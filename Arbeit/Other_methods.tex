\section{Other Methods}
\label{sec: Other methods}

\subsection{Naive Random Walk}
\label{sec: Naive Random Walk}

The Naive Random Walk model is one of the simplest and most comprehensive forecasting models. It assumes that the 1-step difference between two values is i.i.d distributed with mean 0. Its simplicity and easy interpretability makes this method a popular benchmark model. In addition, it is what is currently employed. So using it enables us to directly see if our models outperform the current model. Let $\left\{Y_{kt}:t=1,\ldots T; Y_{kt} \in \mathbb{N}_0\right\}_f$ be again the univariate time series for category $k$ for $k=1,\ldots,K$ and fridge $f$. Then the Naive Random Walk model is given as

\begin{equation}
Y_{k,t+1}= Y_{kt} + \epsilon_{kt},\hspace{0.2cm} k=1,\ldots,K \hspace{0.1cm}, 
\label{eq: Random Walk Model}
\end{equation}
 
where $\epsilon_{kt} \sim WN(\sigma^2)$ is a white noise process with variance $\sigma^2 \in \mathbb{R}_+$. It can be shown easily, that the optimal 1-step ahead forecast with regards to the mean squared error (MSE) is given by

\begin{equation}
\hat{Y}_{k,t+1}= Y_{kt} ,\hspace{0.2cm} k=1,\ldots,K
\label{eq: Random Walk Model Prediction}
\end{equation}

where $\hat{Y}_{k,t+1}$ is the predicted value at time $t$. In other words, the last known value is the predicted value. 


\subsection{Zero-Inflated Models}
\label{sec: Zim}

Since we encounter a large number of zeros we also consider zero-inflated models. Zero inflation means that the proportion of observed zeros is bigger than that of the underlying distribution. The idea of zero-inflated models is to add a degenerated distribution with mass at zero to the probability mass function. This way we can explain the large amount of zero values which otherwise would not be expected in a normal Poisson or negative binomial distribution. The probability mass function of a $ZIP(\lambda,\omega)$ distribution for a random variable $Y$ is defined as \cite{Zhu:2012}

\begin{equation}
\mathbb{P}(Y=y) = \omega \delta_{y,0}+ (1-\omega) \frac{\lambda^y \exp(-\lambda)}{y!}, \hspace{0.2cm} y \in \mathbb{N}_0.
\label{eq:ZIP Distribution}
\end{equation}

where $0 < \omega < 1$ and $\delta_{y,0}$ is the Kronecker delta for which $\delta_{y,0}=1$ if $y=0$ and $\delta_{y,0}=0$ else. This way our zeros can come from two different sources \cite{Zhu:2012}. 

Now we can define the zero-inflated poisson INGARCH(p,q) as

\begin{gather}
\label{eq:ZIP model}
Y_{kt} | \SigA_{k,t-1} \sim ZIP(\lambda_{kt},\omega_k); \forall t \in \mathbb{N}, \\
\mathbb{E}\left[Y_{kt} | \SigA_{k,t-1} \right] = \lambda_{kt} = \beta_0 + \sum_{i=1}^p\beta_i Y_{k,t-i} + \sum_{j=1}^q\alpha_j \lambda_{k,t-j}.
\end{gather}

If $\omega =0$ then we get the normal INGARCH(p,q) model discussed above. It can be shown that the conditional mean and variance are given by

\begin{equation*}
\mathbb{E}[Y_{kt} | \SigA_{k,t-1}] = (1-\omega_k)\lambda_{kt}, \hspace{1cm} \mathbb{V}[Y_{kt} | \SigA_{k,t-1}] =(1-\omega)\lambda_{kt}(1+\omega \lambda_{kt}),
\label{eq:Conditional Mean and Variance ZIP}
\end{equation*}

which implies $ \mathbb{V}[Y_{kt} | \SigA_{k,t-1}] > \mathbb{E}[Y_{kt} | \SigA_{k,t-1}]$ \cite{Zhu:2012}. This means that model \ref{eq:ZIP model} can handle overdispersion in our data. More details about zero-inflated models and especially the zero-inflated INGARCH(p,q) model can be found in \cite{Zhu:2012}.


\subsection{Log-Linear Models}
\label{sec: Log-Linear Models}

As mentioned in \ref{sec:Ingarch Motivation} we also investigate log-linear models. These models are structurally very similar to the normal INGARCH(p,q) model only with a logarithmic link function. They have the form 

\begin{gather}
Y_{kt} | \SigA_{k,t-1} \sim P(\lambda_{kt}); \forall t \in \mathbb{N}, \\
\nu_{kt}= \log(\lambda_{kt}) = \beta_0 + \sum_{i=1}^p\beta_i \log(Y_{k,t-i}+1) + \sum_{j=1}^q\alpha_j \nu_{k,t-j}.
\label{eq:Log-Linear model}
\end{gather}

The past values get transformed by $h(x)=\log(x+1)$ to get them on the same scale as $\nu_{kt}$ and avoid zero values in the logarithm \cite{Liboschik:2016,Fokianos:2011}. We consider the Log-Linear model because it provides solutions to at least two drawbacks from the INGARCH(p,q) model. First, as a result of \ref{eq:Ingarch parameter space}, we have $0 < \sum_{i=1}^p\beta_i + \sum_{j=1}^q\alpha_j < 1$ and hence it follows for $h\in \mathbb{N}$ that $Cov(Y_{k,t+h},Y_{kt})>0$. Second, when we include covariates, they can only have a positive regression term because otherwise the mean $\lambda_{kt}$ becomes negative \cite{Fokianos:2011}. However, in the Log-Linear case we can extend this to

\begin{gather}
Y_{kt} | \SigA_{k,t-1} \sim P(\lambda_{kt}); \forall t \in \mathbb{N}, \\
\nu_{kt}= \log(\lambda_{kt}) = \beta_0 + \sum_{i=1}^p\beta_i \log(Y_{k,t-i}+1) + \sum_{j=1}^q\alpha_j \nu_{k,t-j} + \bm{\eta}^T\textbf{X}_{kt}.
\label{eq:Log-Linear model external factors}
\end{gather}

with $\bm{\eta} \in \mathbb{R}^r$. 

Log-Linear Models are further discussed in \cite{Fokianos:2011,Woodard:2011,Douc:2013}.


\subsection{Vector Generalised Additive Models}
\label{sec:Vgam}

Because we work with multivariate count data, we also look at vector generalised additive models (VGAMs) which extend generalised additive models (GAMs) to higher dimensions. GAMs allow us to reveal and model non-linear relationship in our data, as opposed to linear models or generalised linear models \cite{Yee:1996}. Let $y$ be a univariate response with a distribution in the exponential family and mean $\mu$. Further take a p-dimensional covariate vector and $\bm{x}=(x_1,\ldots,x_p)^T$. Then the generalised additive model (GAM) is given by

\begin{equation}
g(\mu) = \nu(\bm{x}) = \beta_0 + f_1(x_1) + \ldots f_p(x_p),
\label{eq:Gam}
\end{equation}

with $f_j$ being arbitrary smooth functions \cite{Yee:1996}.
To extend this model to the multivariate case, we replace the functions $f_j$ with vector functions. Let $\bm{f}_k(Y_{kt}) = (f_{(1)k}(Y_{kt}),\ldots,f_{(M)k}(Y_{kt}))^T$ with $M \in \mathbbm{N}$ be an arbitrary smooth vector function. Then the vector generalised additive model is given by

\begin{equation}
\mathbb{E}[\bm{Y}_t] = \bm{\beta}_0 + \sum_{k=1}^K\bm{f}_k(Y_{k,t-1}),
\label{eq:Vgam}
\end{equation}

where $\mathbb{E}[\bm{Y}_t] = (\mathbb{E}[Y_{1t}],\ldots,\mathbb{E}[Y_{Kt}])^T$ \cite{Yee:1996}. Further theoretical background about VGAMs are given in \cite{Yee:1996,Yee:2015,Wood:2004}.


\subsection{INAR(p) Models}
\label{sec: Inar Models}

Integer valued autoregressive models of order p (INAR(p)) are another option to handle univariate count data. To define them, we first need to define the generalised thinning operator. Take an integer-valued, non-negative random variable $X$ and $\alpha \in [0,1]$. Further, take a sequence of i.i.d. integer-valued, non-negative random variables $\left\{Z_i \right\}_{i=1}^X$ with finite mean $\alpha$ and variance $\sigma^2<\infty $ which are independent of $X$. Then the generalised thinning operate $\circ$ is defined as

\begin{equation}
\alpha \circ X = \sum_{i=1}^X Z_i .
\label{eq:Thinning operator}
\end{equation}

The sequence  $\left\{Z_i \right\}_{i=1}^X$ is called the counting series of $X$ \cite{Silva:2005}. 

We can then define the INAR(p) model for a positive integer-valued time series $\left\{X_t \right\}$ as

\begin{equation}
X_t = \alpha_1 \circ X_{t-1} + \alpha_2 \circ X_{t-2} + \ldots + \alpha_p X_{t-p} +\epsilon_t ,
\label{eq:Inar(p) model}
\end{equation}

where

\begin{enumerate}
	\item $\left\{\epsilon_t\right\}$ is a sequence of integer-valued i.i.d. random variables with finite first, second and third moment. 
	\item $\alpha_i \circ X_{t-i}$ for $i= 1,\ldots,p$ and $\left\{Z_j \right\}$ for $j=1,\ldots,X_{t-i}$ are mutually independent, independent of $\left\{\epsilon_t\right\}$ and it holds $\mathbb{E}[Z_{i,j}]=\alpha_i$ as well as $\mathbb{V}[Z_{i,j}] = \sigma_i^2$ and $\mathbb{E}[Z_{i,j}^3] = \gamma_i$
	\item $\alpha_i \in [0,1]$ for $i=1,\ldots,p-1$ and $0 < \alpha_p < 1$
	\item $\sum_{j=1}^p \alpha_j < 1$ \cite{Silva:2005}. 
\end{enumerate}


The last condition ensures the existence and stationary of the process. 

Let $\left\{Y_{kt}:t=1,\ldots T; Y_{kt} \in \mathbb{N}_0\right\}_f$ be again the univariate time series for category $k$ for $k=1,\ldots,K$ and fridge $f$. Then the INVAR(p) model is given by

\begin{equation}
Y_{kt} = \alpha_1 \circ Y_{k,t-1} + \alpha_2 \circ Y_{k,t-2} + \ldots + \alpha_p Y_{k,t-p} +\epsilon_{kt}.
\label{eq:Inar(p) model ts}
\end{equation}

For simplicity, we will consider INAR(1) models, although the optimal choice of the lag is something that could be further investigated. 

\subsubsection{Distributional assumptions}
\label{sec: Inar Distributional assumptions}

While we will mainly assume that the innovations $\left\{\epsilon_t\right\}$ follow a Poisson distribution, they can also follow other distributions. One interesting option is, that one can choose a zero-inflated distribution as mentioned in \ref{sec: Zim}. This could make the model adequate for our data. 


\subsubsection{Forecasting}
\label{sec: Inar Forecasting}

The authors in \cite{Silva:2005} present two types of forecasting methods for INAR(1) models. The first approach is a classical method for performing predictions in a time series context and makes use of the conditional expectation. It was obtained by \cite{Bre:1993} and \cite{Freeland:2004}. Assuming that $\left\{\epsilon_t\right\} \sim_{i.i.d} Poisson(\lambda)$, the $h$-step ahead predictor, for $h\in \mathbbm{N}$, based on $n$ past observations $\bm{Y}_k=(Y_{k1},\ldots,Y_{kn})$ is given by

\begin{equation}
\hat{Y}_{k,n+h} | \bm{Y}_k = \mathbb{E}[Y_{k,n+h} | \bm{Y}_k] = \alpha^h \left[Y_{kn}- \frac{\lambda}{1-\alpha} \right] + \frac{\lambda}{1-\alpha}.
\label{eq:Forecasting Classic}
\end{equation}


However, this forecast hardly ever produces integer values. Remedies, like minimising the absolute expected error, have been suggested by \cite{Freeland:2004} but the authors in \cite{Silva:2005} propose a bayesian approach. It is based on the assumption that both, the future prediction $Y_{k,n+h}$ and the vector of unknown parameters $\bm{\theta}=(\alpha,\lambda)$ are random \cite{Silva:2005}. Since the complexity posterior probability density function makes it difficult to work with it directly, a sampling algorithm can be deployed for estimation. The details are again given in \cite{Silva:2005}. The estimator for the conditional expectation is then given by

\begin{equation}
\hat{Y}_{k,n+h} | \bm{Y}_k = Y_{kn}\left(\frac{1}{m} \sum_{i=1}^m\alpha_i^m\right) + \left(\frac{1}{m} \sum_{i=1}^m \frac{1-\alpha_i^h}{1-\alpha_i}\lambda_i\right),
\label{eq:Forecasting Bayesian}
\end{equation}

where $m$ is the sampling size and the pairs $(\alpha_i,\lambda_i)$ for $i=1,\ldots,m$ are the sampled parameters. 