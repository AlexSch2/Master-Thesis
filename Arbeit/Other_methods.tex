\section{Other Methods}
\label{sec: Other methods}

\subsection{Naive Random Walk}
\label{sec: Naive Random Walk}

The Naive Random Walk model is one of the simplest and most comprehensive forecasting models. It assumes that the 1-step difference between two values is i.i.d distributed with mean 0. Its simplicity and easy interpretability makes this method a popular benchmark model. In addition, it is what is currently employed. So using it enables us to directly see if our models outperform the current model. Let $\left\{Y_{kt}:t=1,\ldots T; Y_{kt} \in \mathbb{N}_0\right\}_f$ be again the univariate time series for category $k$ for $k=1,\ldots,K$ and fridge $f$. Then the Naive Random Walk model is given as

\begin{equation}
Y_{k,t+1}= Y_{kt} + \epsilon_{kt},\hspace{0.2cm} k=1,\ldots,K
\label{eq: Random Walk Model}
\end{equation}
 
where $\epsilon_{kt} \sim WN(\sigma^2)$ is a white noise process with variance $\sigma^2 \in \mathbb{R}_+$. It can be shown easily, that the optimal 1-step ahead forecast with regards to the mean squared error (MSE) is given by

\begin{equation}
\hat{Y}_{k,t+1}= Y_{kt} ,\hspace{0.2cm} k=1,\ldots,K
\label{eq: Random Walk Model Prediction}
\end{equation}

where $\hat{Y}_{k,t+1}$ is the predicted value at time $t$. In other words, the last known value is the predicted value. 


\subsection{Zero-Inflated Models}
\label{sec: Zim}

Since we encounter a large number of zeros we also consider zero-inflated models. Zero inflation means that the proportion of observed zeros is bigger than that of the underlying distribution. The idea of zero-inflated models is to add a degenerated distribution with mass at zero to the probability mass function. This way we can explain the large amount of zero values which otherwise would not be expected in a normal Poisson or negative binomial distribution. The probability mass function of a $ZIP(\lambda,\omega)$ distribution for a random variable $Y$ is defined as \cite{Zhu:2012}

\begin{equation}
\mathbb{P}(Y=y) = \omega \delta_{y,0}+ (1-\omega) \frac{\lambda^y \exp(-\lambda)}{y!}, \hspace{0.2cm} y \in \mathbb{N}_0.
\label{eq:ZIP Distribution}
\end{equation}

where $0 < \omega < 1$ and $\delta_{y,0}$ is the Kronecker delta for which $\delta_{y,0}=1$ if $y=0$ and $\delta_{y,0}=0$ else. This way our zeros can come from two different sources \cite{Zhu:2012}. 

Now we can define the zero-inflated poisson INGARCH(p,q) as

\begin{gather}
\label{eq:ZIP model}
Y_{kt} | \SigA_{k,t-1} \sim ZIP(\lambda_{kt},\omega_k); \forall t \in \mathbb{N}, \\
\mathbb{E}\left[Y_{kt} | \SigA_{k,t-1} \right] = \lambda_{kt} = \beta_0 + \sum_{i=1}^p\beta_i Y_{k,t-i} + \sum_{j=1}^q\alpha_j \lambda_{k,t-j}.
\end{gather}

If $\omega =0$ then we get the normal INGARCH(p,q) model discussed above. It can be shown that the conditional mean and variance are given by

\begin{equation*}
\mathbb{E}[Y_{kt} | \SigA_{k,t-1}] = (1-\omega_k)\lambda_{kt}, \hspace{1cm} \mathbb{V}[Y_{kt} | \SigA_{k,t-1}] =(1-\omega)\lambda_{kt}(1+\omega \lambda_{kt})
\label{eq:Conditional Mean and Variance ZIP}
\end{equation*}

which implies $ \mathbb{V}[Y_{kt} | \SigA_{k,t-1}] > \mathbb{E}[Y_{kt} | \SigA_{k,t-1}]$ \cite{Zhu:2012}. This means that model \ref{eq:ZIP model} can handle overdispersion in our data. More details about zero-inflated models and especially the zero-inflated INGARCH(p,q) model can be found in \cite{Zhu:2012}.


\subsection{Log-Linear Models}
\label{sec: Log-Linear Models}

As mentioned in \ref{sec:Ingarch Motivation} we also investigate log-linear models. These models are structurally very similar to the normal INGARCH(p,q) model only with a logarithmic link function. They have the form 

\begin{gather}
Y_{kt} | \SigA_{k,t-1} \sim P(\lambda_{kt}); \forall t \in \mathbb{N}, \\
\nu_{kt}= \log(\lambda_{kt}) = \beta_0 + \sum_{i=1}^p\beta_i \log(Y_{k,t-i}+1) + \sum_{j=1}^q\alpha_j \nu_{k,t-j}.
\label{eq:Log-Linear model}
\end{gather}

The past values get transformed by $h(x)=\log(x+1)$ to get them on the same scale as $\nu_{kt}$ and avoid zero values in the logarithm \cite{Liboschik:2016,Fokianos:2011}. We consider the Log-Linear model because it provides solutions to at least two drawbacks from the INGARCH(p,q) model. First, as a result of \ref{eq:Ingarch parameter space}, we have $0 < \sum_{i=1}^p\beta_i + \sum_{j=1}^q\alpha_j < 1$ and hence it follows for $h\in \mathbb{N}$ that $Cov(Y_{k,t+h},Y_{kt})>0$. Second, when we include covariates, they can only have a positive regression term because otherwise the mean $\lambda_{kt}$ becomes negative \cite{Fokianos:2011}. However, in the Log-Linear case we can extend this to

\begin{gather}
Y_{kt} | \SigA_{k,t-1} \sim P(\lambda_{kt}); \forall t \in \mathbb{N}, \\
\nu_{kt}= \log(\lambda_{kt}) = \beta_0 + \sum_{i=1}^p\beta_i \log(Y_{k,t-i}+1) + \sum_{j=1}^q\alpha_j \nu_{k,t-j} + \bm{\eta}^T\textbf{X}_{kt}.
\label{eq:Log-Linear model external factors}
\end{gather}

with $\bm{\eta} \in \mathbb{R}^r$. 

Log-Linear Models are further discussed in \cite{Fokianos:2011,Woodard:2011,Douc:2013}.


\subsection{Vector Generalised Additive Models}
\label{sec:Vgam}

Because we work with multivariate count data, we also look at vector generalised additive models (VGAMs) which extend generalised additive models (GAMs) to higher dimensions. GAMs allow us to reveal and model non-linear relationship in our data, as opposed to linear models or generalised linear models \cite{Yee:1996}. Let $y$ be a univariate response with a distribution in the exponential family and mean $\mu$. Further take a p-dimensional covariate vector and $\bm{x}=(x_1,\ldots,x_p)^T$. Then the generalised additive model (GAM) is given by

\begin{equation}
g(\mu) = \nu(\bm{x}) = \beta_0 + f_1(x_1) + \ldots f_p(x_p)
\label{eq:Gam}
\end{equation}

with $f_j$ being arbitrary smooth functions \cite{Yee:1996}.
To extend this model to the multivariate case, we replace the functions $f_j$ with vector functions. Let $\bm{f}_k(Y_{kt}) = (f_{(1)k}(Y_{kt}),\ldots,f_{(M)k}(Y_{kt}))^T$ with $M \in \mathbbm{N}$ be an arbitrary smooth vector function. Then the vector generalised additive model is given by

\begin{equation}
\mathbb{E}[\bm{Y}_t] = \bm{\beta}_0 + \sum_{k=1}^K\bm{f}_k(Y_{k,t-1})
\label{eq:Vgam}
\end{equation}

where $\mathbb{E}[\bm{Y}_t] = (\mathbb{E}[Y_{1t}],\ldots,\mathbb{E}[Y_{Kt}])^T$ \cite{Yee:1996}. Further theoretical background about VGAMs are given in \cite{Yee:1996,Yee:2015,Wood:2004}.
