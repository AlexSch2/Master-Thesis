\section{Model Specifications}
\label{sec: Model Specification}

As our data has a specific structure, some transformations can be made to increase performance and stability. The most prominent characteristic of our data is its amount of 0 or null values. As both CoDA and INGARCH can't handle an excessive amount of 0 values, we have to accommodate for this. The concrete way to do this will be described in the following subsections. 

Another varying factor is the history. We define as history the length of the timeseries. While at first it may seem obvious to use as much data as possible, it may actually not always result in a better model. Older values may contain outdated information which influences the estimation the of parameters. Therefore we compare the performance of the models with various history lengths. 

Closely related to the length of the history, is the shape of the window used. The window determines which values are used to estimate the parameters at each point in time. The shape includes both the initial length of it and the way it handles new values. As the different timeseries vary in length, we choose the possible window length as a fraction of the timeseries history. For the way how new values are handled, we focus on two different approaches. The first one uses a fixed window length. This means when a new time point is available, it will be included in the estimation while simultaneously the oldest time point will be removed from the estimation. This has the advantage of only using the most recent and relevant information. The second approach, extends the window at each point in time. When a new value is available, it is included in the estimation of the parameter. With this approach we have more data available at each step and combined with the varying history length we don't have to rely on information that is too old.

\subsection{CoDA Specifications}
\label{sec: Coda Specifications}

As mentioned above the CoDA model must not include any zero values. Since in the CoDA context we see our data as relative data, a value of zero is not defined. Therefore we need to replace them. In order to keep things simple, we consider two options. The first one adds $0.5$ to all timeseries values. The second one only replaces zero values with $0.5$.

As already hinted in the description of the methodology we consider the use of $\Tsp$-Spaces. For this, at each time point, we calculate the total amount and include it as an additional variable in the model. In addition we can choose to take the logarithm of the sum. 

Another characteristic of our data are the low values for some categories of it. Even at the aggregated main category level there are instances with low values for some of the categories. This is the case especially for category 3 and 4. As such, we inspect a method which we will call in the following one-vs-all. The principle is the following. A category is chosen as the pivot category. For all the chosen time points, at each point, the values of the other categories get summed up. Together with the pivot category, the sum of the other categories are then transformed as usual and the VAR model is calculated. All categories are chosen as a pivot category at one point and the predicted values of the pivot groups are then used as the final result. 


\subsection{INGARCH Specifications}
\label{sec: Ingarch Specifications}

As with the CoDA model, we need to take care of the zero values. Again, to keep things simple, we replace missing and zero values with 1. 

As an alternative to the Poisson distribution in \ref{eq:Ingarch Distribution}, a negative binomial distribution can be used as well. This would change \ref{eq:Ingarch Distribution} to 

\begin{equation*}
p_t(y;\bm{\theta})=\mathbb{P}(Y_t=y | \SigA_{t-1}) = \frac{\Gamma(\phi+y)}{\Gamma(y+1)\Gamma(\phi)}\left(\frac{\phi}{\phi+\lambda_t}\right)^\phi\left(\frac{\lambda_t}{\phi+\lambda_t}\right)^y, \hspace{0.2cm} y \in \mathbb{N}_0.
\label{eq:Ingarch negbinom Distribution}
\end{equation*}

With the negative Binomial Distribution the conditional variance is larger than the conditional mean $\lambda_t = \mathbb{V}\left[Y_t | \SigA_{t-1}\right] > \mathbb{E}\left[Y_t | \SigA_{t-1}\right]$.

As seen in the model \ref{eq:Ingarch model with external effect} we can also choose to include external factors or not. However, as our data is of the structure where we don't have information about $\bm{X}_t$ at time $t$, we cannot make use of it. The values $p$ and $q$ are also varying parameters which have to be chosen. 


\subsection{Error Measure}
\label{sec: Error Measure}

In order to compare the results of the methods with each other we will introduce a new error measure. The goal of this measure is to get a performance indicator for each fridge which can be used for comparison and summarisation. Since the scales of the fridges vary, the measure should be scale independent. Because our data contains many zeros, we cannot use a percentage error measure. In addition we want to penalise big absolute difference between the predicted values and actual values. These requirements leads us to the following measure.

For a fridge $j \in \mathbb{N}$, let $i = 1,\ldots,T$ denote the point in time and $k=1,\ldots,K$ the category. Then $y_{jik}$ is the $i$-th true value of the time series for category $k$, $\hat{y}_{jik}$ the predicted value and $y_{n_{ik}}$ the naive predicted value . Then we define our measure as

\begin{equation}
E_j=\frac{\sum_{k=1}^{K}\sum_{i}^T(y_{jik}-\hat{y}_{jik})^2}{\sum_{k=1}^{K}\sum_{i}^T(y_{jik}-y_{naive_{jik}})^2}.
\label{eq: Error Measure}
\end{equation}

With the use of the squared difference we penalise big deviations from the true value. By taking the naive random walk model as a benchmark, we achieve scale independence and are able to compare the performance of our model over different timeseries. This error measure is basically the ration of the mean MSEs for the chosen model and the naive random walk model

\begin{equation}
E_j=\frac{\frac{1}{K}\sum_{k=1}^K MSE_{jk}}{\frac{1}{K}\sum_{k=1}^K MSE_{naive_{jk}}}.
\label{eq: Error Measure MSE}
\end{equation}

If the ratio is below 1, the mean of the MSEs of our methods is lower than that of the naive method and vice versa. This provides a performance indicator for our models. 

\subsubsection{Extension of the Error Measure}
\label{sec:Error Measure Extension}

The measure in \ref{eq: Error Measure} can be further extended. For example, by allowing to use a subset of all possible categories instead of all. Let $G_K \subset \left\{1,\ldots,K\right\}$ then

\begin{equation}
E^{GK}_{j}=\frac{\sum_{k \in G_K}\sum_{i}^T(y_{jik}-\hat{y}_{jik})^2}{\sum_{k \in G_K}\sum_{i}^T(y_{jik}-y_{naive_{jik}})^2}.
\label{eq: Error Measure Subsets}
\end{equation}

This allows us to compare the performance on the subset of categories over various fridges. 

Another possible extension is to take the square root

\begin{equation}
\widetilde{E}_j=\frac{\sum_{k=1}^{K}\sqrt{\sum_{i}^T(y_{jik}-\hat{y}_{jik})^2}}{\sum_{k=1}^{K}\sqrt{\sum_{i}^T(y_{jik}-y_{naive_{jik}})^2}}.
\label{eq: Error Measure Sqrt} 
\end{equation}

One future extension which can be investigated is the introduction of weights. This could be used for example when the performance of the model in one category should be put more into focus. 