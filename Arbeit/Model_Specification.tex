\section{Model Specifications}
\label{sec: Model Specification}

As our data has a specific structure, some transformations can be made to increase performance and stability. The most prominent characteristic of our data is its amount of 0 or null values. As CoDA can't handle an excessive amount of 0 values, we have to accommodate for this. The concrete way to do this will be described in the following subsections. 

Another varying factor is the history. We define the history $h$ as the proportion of the length of the time series used for our model. While at first it may seem obvious to use as much data as possible, it may actually not always result in a better model. Older values may contain outdated information, which influences the estimation the of parameters. Therefore we compare the performance of the models with various history lengths. So instead of using $T_f$ points in time, we will only use $T=h\cdot T_F$ with $0 < h \leq 1$. 

Closely related to the length of the history, is the shape of the window used. The window determines which values are used to estimate the parameters at each point in time. The shape includes both the initial length of the window and the way new values are handled. As the different time series vary in length, we choose the possible window length as a fraction of the time series history. Hence, we define the initial window length as $w_f=w \cdot T$ with $0 < w \leq 1$. For the way how new values are handled, we focus on two different approaches. The first one uses a fixed window length. This means when a new time point is available, it will be included in the estimation while simultaneously the oldest time point will be removed from the estimation. This has the advantage of only using the most recent and relevant information. The second approach extends the window at each point in time. When a new value is available, it is included in the estimation of the parameter. With this approach we have more data available at each step and combined with the varying history length we don't have to rely on information that is too old.

The optimal one step ahead prediction for the different models is given in their respective theoretical sections. However, since none of the models return integer-valued results, we round the predicted values to the nearest integer. 

\subsection{CoDA Specifications}
\label{sec: Coda Specifications}

As mentioned above, the CoDA model must not include any zero values, since in this context, a value of zero is not defined. In order to keep things simple, we consider two options. The first one adds $0.5$ to all time series values. The second one only replaces zero values with a chosen value $\delta$, which is the simple replacement strategy in (\ref{eq:simple replacement strategy}). Due to the fact that we have essential zeros and want to use the specific $ilr$ coordinates, we opt for these options. 

Another way to handle the zero values and the low values for some categories is a method we will call in the following one-vs-all. The principle is the following. A category $k$ is chosen as the pivot category $k_{pivot}$. For all the chosen time points, at each point, the values of the other categories get summed up

\begin{equation}
Y_{other,t} = \sum_{\substack{k=1 \\ k \neq k_{pivot}}}^K Y_{kt}.
\label{eq:one vs all}
\end{equation}
%
Together with the pivot category, the sum of the other categories are then transformed as usual and the VAR model is calculated 

\begin{equation}
\bm{u}_t = ilr([Y_{other,t}, Y_{k_{pivot},t}]).
\label{eq:one vs all ilr}
\end{equation}
%
All categories are chosen as a pivot category at one point and the predicted values of the pivot groups are then used as the final result. This method is basically an implementation of the suggestions made in \textcite{Aitchison:2003}. We amalgamate all but one category and therefore change the experimental design. 

As already hinted in the description of the methodology, we consider the use of $\Tsp$-Spaces. For this, at each time point, we calculate the total amount and include it as an additional variable in the model. In addition, we can choose to take the logarithm of the sum. This means we have 

\begin{equation}
\bm{w}_t = [ilr(\bm{Y}_t),t(\bm{Y}_t)],
\label{eq:Tspace u}
\end{equation}
%
with $t(\bm{Y}_t) = \sum_{k=1}^K Y_{kt}$ or $t(\bm{Y}_t) = \log\left(\sum_{k=1}^K Y_{kt}\right)$ and we get Model (\ref{eq:VAR model Tspace}).

%The optimal one step ahead prediction is described in section \ref{sec: AR Estimation and Forecasting}. However, since we only have integer-valued data, we round the predicted values to the nearest integer. 

\subsection{INGARCH Specifications}
\label{sec: Ingarch Specifications}

As an alternative to the Poisson distribution in (\ref{eq:Ingarch Distribution}), a Negative Binomial distribution can be used as well. This would change (\ref{eq:Ingarch Distribution}) to 

\begin{equation}
p_{kt}(y;\bm{\theta})=\mathbb{P}(Y_{kt}=y | \SigA_{k,t-1}) = \frac{\Gamma(\phi_k+y)}{\Gamma(y+1)\Gamma(\phi_k)}\left(\frac{\phi_k}{\phi_k+\lambda_{kt}}\right)^{\phi_k}\left(\frac{\lambda_{kt}}{\phi_k+\lambda_{kt}}\right)^y, \hspace{0.2cm} y \in \mathbb{N}_0.
\label{eq:Ingarch negbinom Distribution}
\end{equation}
%
where $\phi_k$ is the dispersion parameter, $\lambda_{kt}$ the mean and $\Gamma$ is the gamma function. 
With the Negative Binomial Distribution, the conditional variance is larger than the conditional mean $\lambda_{kt} = \mathbb{V}\left[Y_{kt} | \SigA_{k,t-1}\right] > \mathbb{E}\left[Y_{kt} | \SigA_{k,t-1}\right]$.

As seen in the Model (\ref{eq:Ingarch model with external effect}), we can also choose to include external factors. However, as our data is of the structure where we don't have information about $\bm{X}_t$ at time $t$, we cannot make use of it. 

The values $p$ and $q$ are also varying parameters which have to be chosen. One could use the AIC or some other criteria to get the optimal lag order. However, this is not in the scope of this thesis and hence will be left as a future extension.

%The optimal one step ahead prediction is given by the conditional expectation \ref{sec: Estimation of the Ingarch Model}. However, since we only expect integer values and $\lambda_{k,t+1} \in \mathbbm{R}_+$, we will round the values of $\lambda_{k,t+1}$ to the next integer and use this value. 



\section{Error Measure}
\label{sec: Error Measure}

In order to compare the results of the methods with each other, we will introduce a new error measure. The goal of this measure is to get a performance indicator for each fridge, which can be used for comparison and summarisation. Since the scales of the fridges vary, the measure should be scale independent but because our data contains many zeros, we cannot use a percentage error measure. In addition, we want to penalise big absolute differences between the predicted values and actual values. These requirements lead us to the following measure.

For a fridge $f$, let $t = 1,\ldots,T$ denote the point in time and $k=1,\ldots,K$ the category. Then $y_{ftk}$ is the $t$-th measured value of the time series for category $k$ and fridge $f$, $\hat{y}_{ftk}$ the predicted value and $y_{naive_{ftk}}$ the value predicted by the naive random walk model (\ref{sec: Naive Random Walk}). Then we define our measure as

\begin{equation}
E_f=\frac{\sum_{k=1}^{K}\sum_{t=1}^T(y_{ftk}-\hat{y}_{ftk})^2}{\sum_{k=1}^{K}\sum_{t=1}^T(y_{ftk}-\hat{y}_{naive_{ftk}})^2}.
\label{eq: Error Measure}
\end{equation}
%
With the use of the squared difference, we penalise big deviations from the measured value. By taking the naive random walk model as a benchmark, we achieve scale independence and are able to compare the performance of our model over different time series. This error measure is basically the ratio of the mean MSEs for the chosen model and the naive random walk model

\begin{equation}
E_f=\frac{\frac{1}{K}\sum_{k=1}^K MSE_{fk}}{\frac{1}{K}\sum_{k=1}^K MSE_{naive_{fk}}}.
\label{eq: Error Measure MSE}
\end{equation}
%
If the ratio is below 1, the mean of the MSEs of our method is lower than that of the naive method and vice versa. This provides a performance indicator for our models. 

\subsubsection{Extension of the Error Measure}
\label{sec:Error Measure Extension}

The measure in (\ref{eq: Error Measure}) can be further extended. For example, by allowing to use a subset of all possible categories instead of all. Let $G_K \subset \left\{1,\ldots,K\right\}$ then

\begin{equation}
E^{GK}_{f}=\frac{\sum_{k \in G_K}\sum_{t=1}^T(y_{ftk}-\hat{y}_{ftk})^2}{\sum_{k \in G_K}\sum_{t=1}^T(y_{ftk}-y_{naive_{ftk}})^2}.
\label{eq: Error Measure Subsets}
\end{equation}
%
This allows us to compare the performance on the subset of categories over various fridges. 

Another possible extension is to take the square root

\begin{equation}
\widetilde{E}_f=\frac{\sum_{k=1}^{K}\sqrt{\sum_{t=1}^T(y_{ftk}-\hat{y}_{ftk})^2}}{\sum_{k=1}^{K}\sqrt{\sum_{t=1}^T(y_{ftk}-y_{naive_{ftk}})^2}}.
\label{eq: Error Measure Sqrt} 
\end{equation}
%
One future extension which can be investigated is the introduction of weights. This could be used for example when the performance of the model in one category should be put more into focus. 