\subsection{Motivation}
\label{sec: Coda Motivation}

One way to see our data is as a compositional time series. The exact definition of compositional will follow later but in general compositional data, which is by nature multivariate, describes relations between the parts instead of absolute values. We transform the data in such a way, that the values of each category can be seen as the relative share of the total amount at the current time. We then predict the relative share of the category for the next point in time. Since we are ultimately interested in the absolute value, we include the total sum of all categories as an additional variable and then use it for calculating the absolute value. This is modelled as the so-called $\Tsp$-Space which will be introduced later. Since VAR models are easy to estimate and interpret and have some beneficial properties with our choice of transformation, we opt to focus on them. One such property is the fact that the VAR model does not depend on the concrete choice of the ilr-transformation \cite{Kynclova:2015}. 

%Beschreiben warum CoDA und wie unsere Daten als compositional Data gesehen werden kann. 

\section{Preliminaries}
\label{sec: Coda Preliminaries}
The basis of this section is given by \cite{Kynclova:2015}, \cite{Egozcue:2003} and \cite{Filzmoser:2020}.

CoDA, which is short for "`Compositional Data Analysis"' works with compositional data. The key to compositional data is the fact that the absolute value of its parts is less important than the relative relation of the parts to each other. To define compositional data, we first need to define the $(D-1)$-dimensional simplex,
	\begin{equation}
	\mathbbm{S}^D := \left\{(x_1,\ldots, x_D)^T: x_i > 0, i =1,\ldots,D; \sum_{i=1}^{D}x_i=\kappa  \right\},
	\label{eq:Simplex Definition}
	\end{equation}
	
where $\kappa$ is a positive constant \cite{Kynclova:2015}. The choice of $\kappa$ is not relevant, as the relative information in the compositional parts stays the same.  A D-dimensional vector $\textbf{x} = (x_1,\ldots,x_D)^T$ is said to be compositional if it is part of $\mathbbm{S}^D$. Next we can induce a $(D-1)$-dimensional vector space on $\mathbbm{S}^D$ by perturbation and power transformation. For compositions $\textbf{x},\textbf{z} \in \mathbbm{S}^D$ and $a \in \mathbb{R}$ they are defined respectively as \cite{Kynclova:2015}

\begin{equation}
\textbf{x}\oplus_a \textbf{z} := \ClOp(x_1z_1,x_2z_2,\ldots,x_Dz_D)^T, \hspace{0.5cm} a \odot_a \textbf{x} := \ClOp(x_1^a,x_2^a,\ldots,x_D^a)^T.
\label{eq:Simplex Operationen}
\end{equation}

Here $\ClOp$ is the closure operation that maps each compositional vector from the real value space $\mathbbm{R}_+^D$ into its representation in $\mathbbm{S}^D$

\begin{equation}
\ClOp \left(\bm{x}\right) := \left(\frac{\kappa x_1}{\sum_{i=1}^D x_i},\ldots, \frac{\kappa x_D}{\sum_{i=1}^D x_i}\right)^T.
\label{eq:Closure Operation}
\end{equation}


Using $z^{-1} := \ClOp(z_1^{-1},z_2^{-1},\ldots,z_D^{-1})$, the inverse perturbation can be defined as 

\begin{equation}
\textbf{x} \ominus_a \textbf{z} := \textbf{x} \oplus_a \textbf{z}^{-1},
\label{eq: Inverse Perturbation}
\end{equation}

Now we further define an inner product in order to have an inner product space over the simplex $\mathbbm{S}^D$. For two compositions $\textbf{x},\textbf{z} \in \mathbbm{S}^D$ define the Aitchison inner product as 

\begin{equation}
\left\langle \textbf{x},\textbf{z} \right\rangle_a := \frac{1}{2D}\sum_{i=1}^{D}\sum_{j=1}^{D}\log(\frac{x_i}{x_j})\log(\frac{z_i}{z_j}).
\label{eq:Aitchon inner product}
\end{equation}

%With this, the Euclidean vector space structure is induced on the simplex. 
In addition, a norm and distance measure can be defined

\begin{equation}
\norm{\textbf{x}}_a^2 := \left\langle  \textbf{x},\textbf{x} \right\rangle_a, \hspace{0.5cm} d_a(\textbf{x},\textbf{z}) := \norm{\textbf{x} \ominus_a \textbf{z}}_a.
\label{eq:Simplex Norm and Distance}
\end{equation}

This induced geometry is called the Aitchison geometry and it allows us to express a composition $\textbf{x} \in \mathbbm{S}^D$ as a perturbation-linear combination of a basis of $\mathbbm{S}^D$.

However, in order to use standard statistical tools, it is desirable to move from this geometry to the Euclidean real space \cite{Filzmoser:2020}. There are various ways to map the data from the simplex $\mathbbm{S}^D$ to the real space $\mathbb{R}^D$. A review of the most common transformations is provided in the following section.

\section{Common Transformations}
\label{sec: Common Transformations}

Let $\textbf{x}, \textbf{z} \in \mathbbm{S}^D$ be D-part compositions.
\subsubsection{alr Coordinates}
\label{sec:alr Coordinates}

The additive log-ratio (alr) Coordinates are defined as \cite{Kynclova:2015}

\begin{equation}
\textbf{z}^{(k)} = alr_k(\textbf{x}) := \left(\log\left(\frac{x_1}{x_k}\right), \ldots, \log\left(\frac{x_{k-1}}{x_k}\right),\log\left(\frac{x_{k+1}}{x_k}\right),\ldots,\log\left(\frac{x_D}{x_k}\right)\right).
\label{eq:alr Coordinates}
\end{equation}

and map the composition $\textbf{x}$ to the real space $\mathbb{R}^D$. They are mainly mentioned for historic purposes since they are an intuitive way of transformation. However, limitations are posed by their dependence on the choice of the denominator $x_k$ and the fact that they are not orthogonal to each other \cite{Filzmoser:2020}. 

\subsubsection{clr Coefficients}
\label{sec:clr Coefficients}

Let $g(\textbf{x})$ be the geometric mean of $\textbf{x}$. The centered log-ratio coefficients are then defined as \cite{Kynclova:2015}

\begin{equation}
\textbf{w} = (w_1,\ldots, w_D)^T = clr(\textbf{x}) := \left(\log\left(\frac{x_1}{g(\textbf{x})}\right),\ldots, \log\left(\frac{x_D}{g(\textbf{x})}\right)\right)^T.
\label{eq:clr Coefficients}
\end{equation}

This transformation maps $\textbf{x}$ into the hyperplane $V = \left\{\textbf{w} \in \mathbb{R}^D: \sum_{i=1}^D w_i=0\right\} \subset \mathbb{R}^D$. Hence the transformed data is constrained, which is emphasised by the term 'coefficient' instead of 'coordinates' \cite{Filzmoser:2020}. It can be shown that the $clr$ transformation is an isometry\cite{Egozcue:2003}. Therefore it holds 

\begin{gather}
\left\langle  \textbf{x},\textbf{z} \right\rangle_a = \left\langle  clr(\textbf{x}),clr(\textbf{z}) \right\rangle_a, \\
d(\textbf{x},\textbf{z})_a = d(clr(\textbf{x}),clr(\textbf{z})).
\label{eq:clr Coefficients isometric}
\end{gather}

\subsubsection{ilr Coordinates}
\label{sec:ilr Coordinates}

The isometric log-ratio (ilr) are closely related to the clr Coefficients. Assume the inverse $clr$ transformation is isometric. Let $\left\{v_1,\ldots,v_{D-1}\right\}$ be an orthonormal base in the hyperplane $V$. Then $\textbf{e}_i = clr^{-1}(v_i), i=1,\ldots,D-1$ is an orthonormal basis of the simplex $\mathbbm{S}^D$. For $\textbf{x} \in \mathbbm{S}^D$, the $ilr$ transformation can then be defined as \cite{Kynclova:2015}

\begin{equation}
\textbf{u} = ilr(\textbf{x}) = \left(\left\langle \textbf{x},\textbf{e}_1\right\rangle_a,\ldots,\left\langle \textbf{x},\textbf{e}_{D-1}\right\rangle_a\right)^T.
\label{eq:ilr Coordinates}
\end{equation}

In addition to being isometric, the $ilr$ transformation is also isomorph. Let $\textbf{x}, \textbf{z}$ be two compositions and $a, b  \in \mathbbm{R}$. Then,

\begin{equation}
ilr(a \odot \textbf{x} \oplus_a b \odot_a \textbf{z}) = a \cdot ilr(\textbf{x}) + b \cdot ilr(\textbf{z}),
\label{eq:ilr coordinates isomorph}
\end{equation}

as well as,

\begin{gather}
\left\langle  \textbf{x},\textbf{z} \right\rangle_a = \left\langle  ilr(\textbf{x}), ilr(\textbf{z}) \right\rangle_a, \\
d(\textbf{x},\textbf{z})_a = d(ilr(\textbf{x}),ilr(\textbf{z})),\\
\norm{x}_a = \norm{ilr(x)} = \norm{u}.
\label{eq:ilr coordinates isometric }
\end{gather}


From the definition of the ilr coordinates it can be seen that they can be expressed as a linear combination of the basis induced by the clr coefficients as seen above. Let $\textbf{V}$ be a $D \times (D-1)$ matrix with columns $\textbf{v}_i = clr(\textbf{e}_i)$. For a composition $\textbf{x}$ the vector of ilr coordinates associated with $\textbf{V}$ is given by,

\begin{equation}
\textbf{u}_{\textbf{V}} = ilr_{\textbf{V}}(\textbf{x}) = \textbf{V}^T clr(\textbf{x}) = \textbf{V}^T \log(\textbf{x}). 
\label{eq:ilr coordinates with V}
\end{equation}

The matrix $\textbf{V}$ is the contrast matrix with the orthonormal basis $(\textbf{e}_i)_{i=1}^{D-1}$ \cite{Egozcue:2003}. A special choice of orthogonal coordinates leads to the coordinates 

\begin{gather}
ilr(\textbf{x}) = (u_1,\ldots,u_{D-1})^T, \\
u_j = \sqrt{\frac{D-j}{D-j+1}}\log\left(\frac{x_j}{\sqrt[D-j]{\prod_{l=j+1}^D x_l}}\right), \hspace{0.2cm} j=1,\ldots, D-1.
\label{eq:pivot coordinates}
\end{gather}

With this choice, the problem of interpretation, which arises from the relative nature of the compositional data and the dimension of the simplex, can be solved. The part $x_1$ is only contained in $z_1$ and therefore contains all relative information of $x_1$ \cite{Filzmoser:2020}. 

To transform the data back in the simplex, the inverse transformation is given by, 

\begin{gather}
x_1 = \exp\left(\sqrt{\frac{D-1}{D}}u_1\right), \\
x_i = \exp\left( \sum_{j=1}^{i-1}\frac{1}{\sqrt{(D-j+1)(D-j)}}u_j + \sqrt{\frac{D-i}{D-i+1}u_i} \right), \hspace{0.2cm} i=2,\ldots, D-1, \\
x_D = \exp\left(- \sum_{j=1}^{D-1} \frac{1}{\sqrt{(D-j+1)(D-j)}}u_j \right).
\label{eq: Inverse Transformation}
\end{gather}


\section{The VAR Model}
\label{sec:The VAR Model}

Since we have established the basic setting we can now introduce compositional time series (CTS). A CTS $\left\{\bm{x}_t:t=1,\ldots,n \right\}$ can be defined as a series where $\bm{x}_t = \left(x_{1t},\ldots,x_{Dt}\right)^T \in \mathbbm{S}^D$. They are thus characterised by their positive components which sum up to a constant $\kappa_t$ for each point in time $t=1,\ldots,n$ 

\begin{equation}
\sum_{i=1}^D x_{it} = \kappa_t, \hspace{0.2cm} x_i > 0, i=1,\ldots D \hspace{0.1cm}; t=1,\ldots, n. 
\label{eq:CTS characterisation}
\end{equation} 

Let $\left\{\bm{Y}_t:t=1,\ldots,T; \bm{Y}_t \in \mathbb{N}_0^K \right\}_f$ be our time series for fridge $f$ and assume that $\bm{Y}_t=(Y_{1t},\ldots,Y_{Kt})^T$ is a K-dimensional compositional vector measured at time $t, t=1,\ldots,T$. Further, let $\textbf{u}_t = ilr(\bm{Y}_t)$ be its $ilr$ transformation determined by the matrix $\textbf{V}$. Then the VAR model with lag order $p$ is given by \cite{Kynclova:2015}

\begin{equation}
\textbf{u}_t = \textbf{c}_{\textbf{V}} + \textbf{A}_{\textbf{V}}^{(1)}\textbf{u}_{t-1} + \textbf{A}_{\textbf{V}}^{(2)}\textbf{u}_{t-2} + \ldots + \textbf{A}_{\textbf{V}}^{(p)}\textbf{u}_{t-p} + \bm{\epsilon}_{t}.
\label{eq:VAR model}
\end{equation}

where $\textbf{c}_{\textbf{V}} \in \mathbb{R}^{K-1}$ is a real vector, $\textbf{A}_{\textbf{V}}^{(i)} \in \mathbb{R}^{(K-1) \times (K-1)}$ are parameter matrices and $\bm{\epsilon}_t$ is a white noise process with covariance matrix $\bm{\Sigma_\epsilon}$. The observation $\textbf{u}_t$ therefore depends on the p past observations $\textbf{u}_{t-1},\ldots,\textbf{u}_{t-p}$. It can be shown, that two VAR(p) models resulting from different $ilr$ transformations are compositionally equivalent, which means that the same predictions are obtained \cite{Kynclova:2015}. 


\subsubsection{Estimation of the VAR Model}
\label{sec: Estimation of the Var Model}


Assuming $T$ observations are used for the model, equation \ref{eq:VAR model} can be written in matrix form as

\begin{gather*}
\textbf{U} = \textbf{ZB} + \textbf{E}, \\
\textbf{U} = (\textbf{u}_1,\ldots,\textbf{u}_T)^T \in \mathbb{R}^{T \times (K-1)}, \\
\textbf{Z} \in \mathbb{R}^{T \times \left[(K-1)p+1\right]} \text{ with } \textbf{Z}_t = \left(1,\textbf{u}_{t-1}^T,\ldots,\textbf{u}_{t-p}^T\right)^T, \\ %von \cite{Kynclova:2015}
\textbf{B} = \left[\textbf{c},\textbf{A}^{(1)}, \ldots, \textbf{A}^{(p)}\right]^T \in \mathbb{R}^{(K-1)p +1 \times (K-1)}.
\label{eq:VAR model matrix}
\end{gather*}


The parameter $\textbf{B}$ can then be estimated separately for each column of $\textbf{U}$ by the ordinary least squares (OLS) method. In addition, if there are no restrictions posed on the parameter, the estimator is equal to the generalised least squares (GLS). If the VAR(p) process is normally distributed and the rows of the error matrix $\textbf{E}$ represent a white noise process, thus $\bm{E} \sim WN(\Sigma)$ where $\Sigma$ is the covariance matrix, then the estimator is also equal to the maximum likelihood (ML) estimator. 
Under these assumptions it can be shown that the OLS estimator is consistent and asymptotic normal \cite{Kynclova:2015} \cite{Luetkepohl:2007}. 


\section{$\Tsp$-Spaces}
\label{sec: Tspaces}

As we have seen lies the focus in compositional data analysis in the relative information encoded in the observations. However, as is often the case in practice, the absolute information is of interest as well. To retain this information, usually two practices are used. First, for a vector $\bm{x} \in \mathbb{R}^D_+$ the component wise logarithm $\log(\bm{x})$ is considered. Second, the total sum, or some other function, of $\bm{x}$ is added as an additional variable \cite{Pawlowsky:2013}. Here, we will dive deeper into the second method mentioned. An overview over the first method can be found in \cite{Pawlowsky:2013}. 

Let $\bm{x} \in \mathbb{R}^D_+$ be a positive vector and $\ClOp(\bm{x})$ the projection onto $\mathbbm{S}^D$. Further, take a function $t:\mathbb{R}^D_+ \longrightarrow \mathbb{R}_+$ (i.e. the sum, product,...). Then define the product space  $\Tsp = \mathbb{R}_+ \times \mathbbm{S}^D$ as the space of all possible elements $[t(\bm{x}),\ClOp(\bm{x})]$ \cite{Pawlowsky:2013}. To define a D-dimensional Euclidean vector space structure on $\Tsp$ we define an Abelian inner group operation, an external multiplication, and an inner product \cite{Pawlowsky:2013}. However, first we need to induce the Euclidean structure on $\mathbb{R}_+^D$ with the same operations. For $\bm{x},\bm{y} \in \mathbbm{R}_+^D$ and $\alpha \in \mathbbm{R}$ define the Abelian inner group operation, the external multiplication, and an inner product respectively as \cite{Pawlowsky:2013}

\begin{gather}
\bm{x} \oplus_+ \bm{y}:= [x_1\cdot y_1,\ldots,x_D \cdot y_D)], \\
\alpha \odot_+ \bm{x} := [x_1^{\alpha},x_D^{\alpha}], \\
\left\langle \bm{x},\bm{y} \right\rangle_+ := \left\langle \log(\bm{x}),\log(\bm{y}) \right\rangle.
\label{eq:Operations on Rdplus}
\end{gather}

Here, $\left\langle ,\right\rangle$ denotes the usual Euclidean inner product on $\mathbbm{R}^D$. 

Now we can define for $\tilde{\bm{x}},\tilde{\bm{y}} \in \Tsp$ and $\alpha \in \mathbbm{R}$ the Abelian inner group operation as 

\begin{equation}
\tilde{\bm{x}} \oplus_T \tilde{\bm{y}} = [t(\bm{x}) \oplus_+ t(\bm{y}), \bm{x} \oplus_a \bm{y}] := [t(\bm{x}) \cdot t(\bm{y}), \ClOp(\tilde{x}_1\tilde{y}_1,\ldots,\tilde{x}_D\tilde{y}_D)],
\label{eq:Abelian inner group operation}
\end{equation}

and the external multiplication as 

\begin{equation}
\alpha \odot_T \tilde{\bm{x}} = [\alpha \odot_+ t(\bm{x}), \alpha \odot_a \bm{x}] := [t(\bm{x})^{\alpha},\ClOp(\tilde{x}_1^{\alpha},\tilde{x}_D^{\alpha}],
\label{eq:external multiplication}
\end{equation}

where $\oplus_a$ and $\odot_a$ are the perturbation and power transformation defined in \ref{eq:Simplex Operationen} and $\oplus_+$ and $\odot_+$ the respective operations defined for $\mathbbm{R}_+$ \ref{eq:Operations on Rdplus}.

The inner product is defined as 

\begin{equation}
\left\langle \tilde{\bm{x}},\tilde{\bm{y}} \right\rangle_T := \left\langle t(\bm{x}),t(\bm{y}) \right\rangle_+ + \left\langle \ClOp(\bm{x}),\ClOp(\bm{y}) \right\rangle_a,
\label{eq:inner product Tspace}
\end{equation}

where $\left\langle ,\right\rangle_+$ is the inner product in $\mathbbm{R}_+$, and $\left\langle ,\right\rangle_a$ is the Aitchison inner product defined in \ref{eq:Aitchon inner product} \cite{Pawlowsky:2013}.

Further we can define a distance on $\Tsp$ with 

\begin{gather}
d_T^2(\tilde{\bm{x}},\tilde{\bm{y}}) = d_+^2(t(\bm{x}),t(\bm{y})) + d_a^2(\ClOp(\bm{x}),\ClOp(\bm{y})),
\label{eq:distance Tspace}
\end{gather}

with $d_+^2(\bm{x},\bm{y}) = d(\log(\bm{x}),\log(\bm{y}))$ and $d$ is the Euclidean distance. 

To ensure that the operations performed on $\ClOp(\bm{x})$ are compatible with the ones performed on $\Tsp$ we need to impose some conditions on the function $h:\mathbbm{R}_+^D \rightarrow \Tsp$, $h(\bm{x}):=[t(\bm{x}),\ClOp(\bm{x})]$. First, the function $h$ needs to be a one-to-one function since otherwise information could be lost by applying $h$ or $h^{-1}$. A result of this is the fact, that the function $t$ must be related to the sum of the components. To see this, write $\bm{x} \in \mathbbm{R}_+^D$ as $\bm{x} = \frac{\sum_{i=1}^D x_i}{\kappa} \cdot \ClOp(\bm{x})$. Hence $ \frac{\sum_{i=1}^D x_i}{\kappa} \cdot \ClOp(\bm{x}) = h^{-1}([t(\bm{x}),\ClOp(\bm{x})])$ \cite{Pawlowsky:2013}.
The second condition is the preservation of the vector space properties in $\mathbbm{R}_+^D$ and $\Tsp$

\begin{gather}
h(\bm{x} \oplus_+ \bm{y}) = h(\bm{x}) \oplus_T h(\bm{y}), \\
h(\alpha \odot_T \bm{x}) = \alpha \odot_T h(\bm{x}). 
\label{eq:Vector Space Properties}
\end{gather}

This means for the function $t$ that 

\begin{gather}
t(\bm{x} \oplus_+ \bm{y}) = t(\bm{x}) \cdot t(\bm{y}), \\
t(\alpha \odot_T \bm{x}) = [t(\bm{x})]^{\alpha}. 
\label{eq:Vector Space Properties for t}
\end{gather}

In \cite{Pawlowsky:2013} the authors show that for $h_s=([t_s(\bm{x}),\ClOp(\bm{x})])$ with $t_s(\bm{x}) = \sum_{i=1}^D x_i$ is a one-to-one function, but not compatible with $\oplus_+,\odot_+$ and $\oplus_T,\odot_T$. However, as $h_s$ is a one-to-one function between $\mathbbm{R}_+^D$ and $\Tsp$, there exists a Euclidean structure in $\mathbbm{R}_+^D$ that is isometric to the one in $\Tsp$ \cite{Pawlowsky:2013}. The vector space operations can be defined as

\begin{gather}
\bm{x} \oplus_{+s} \bm{y} = h_s^{-1}(\tilde{\bm{x}}) \oplus_T  h_s^{-1}(\tilde{\bm{y}}), \\
\alpha \odot_{+s} \bm{x} = \alpha \odot_T h_s{^-1}(\tilde{\bm{x}}), \\
d_{+s}^2(\bm{x},\bm{y}) = d_T^2(h_s(\bm{x},\bm{y})),
\label{eq:Vector Space Operations sum}
\end{gather}

where $\oplus_{+s}$ and $\odot_{+s}$ are the new operations in $\mathbbm{R}_+^D$ and $d^2$ is the squared distance in $\Tsp$. 

In our case, we have again $\bm{Y}_t=(Y_{1t},\ldots,Y_{Kt})^T$ and hence $\Tsp = \mathbb{R}_+ \times \mathbbm{S}^K$. So $\tilde{\bm{Y}}_t=h(\bm{Y}_t)=[t(\bm{Y}_t),\ClOp(\bm{Y}_t)]$ with $t(\bm{Y}_t)=\sum_{k=1}^K Y_{kt}$. For $\bm{w}_t =(t(\bm{Y}_t),ilr(\bm{Y}_t))^T$ take the $irl$ transformation determined by matrix $\bm{V}$. Further, let $\bm{c}_{\bm{V}} \in \mathbbm{R}^K$ be a real vector, $\textbf{A}_{\textbf{V}}^{(i)} \in \mathbb{R}^{K \times K}$ parameter matrices and $\bm{\epsilon}_t$ be a white noise process with covariance matrix $\bm{\Sigma_\epsilon}$

\begin{equation}
\textbf{w}_t = \textbf{c}_{\textbf{V}} + \textbf{A}_{\textbf{V}}^{(1)}\textbf{w}_{t-1} + \textbf{A}_{\textbf{V}}^{(2)}\textbf{w}_{t-2} + \ldots + \textbf{A}_{\textbf{V}}^{(p)}\textbf{w}_{t-p} + \bm{\epsilon}_{t}.
\label{eq:VAR model Tspace}
\end{equation}

In our application we will use $t(\bm{Y}_t)=\sum_{k=1}^K Y_{kt}$ or $t(\bm{Y}_t)=\log(\sum_{k=1}^K Y_{kt})$ since we are interested in the total sum at time $t$. The logarithmic sum is a popular choice in the time series context \cite{Kynclova:2015}. 

\section{Zero-Handling}
\label{sec: Zero-Handling}

As we can see in the definition of the simplex \ref{eq:Simplex Definition}, a compositional vector can only consist of positive parts. Since we have a considerate amount of zeros in our data, we need to take care of them. There have been various methods proposed in literature to handle zero values in compositional data. First, however, a distinction must be made in the type of zeros present. One can differentiate between two types of zeros. The first type of zeros is called structural zeros or essential zeros. Those values are truly zero. The second type is called rounded zeros or count zeros. They appear due to imprecision when measuring data or if the detected value is below the detection limit. Those values are not truly zero and hence it makes sense to replace them in order to perform compositional data analysis. In the following we summarise the methods presented in \cite{Lubbe:2021,Josep:2003}. 


Let $\bm{x} \in \mathbbm{S}^D$ be a compositional vector and assume it has $m$ zeros. Further take $\bm{r} \in \mathbbm{S}^D$ as its zero free replacement. If we have rounded zeros, a simple method proposed in \cite{Josep:2003} is to replace zero values with $DL \cdot 0.65$ where DL is the detection limit and 0.65 was found to be optimal to minimise the distortion in the covariance structure \cite{Lubbe:2021}. This means $\bm{r}$ has the form

\begin{equation}
r_j = 
\begin{cases}
0.65\cdot DL, & \text{if} x_j=0, \\
x_j, & \text{if} x_j>0, 
\end{cases}
\label{eq:DL065}
\end{equation}

Additionally \cite{Josep:2003} mentions two other methods. First, the Additive Replacement Strategy, which was first introduced by Aitchison in \cite{Aitchison:1986}, and is given by

\begin{equation}
r_j = 
\begin{cases}
\frac{\delta(m+1)(D-m)}{D^2}, & \text{if } x_j=0, \\
x_j - \frac{\delta(m+1)m}{D^2}, & \text{if } x_j>0.
\end{cases}
\label{eq:additive replacement strategy}
\end{equation}

Second, the Simple Replacement Strategy, which formalises the procedure of replacing the zeros in $\bm{x}$ with a small positive value $\delta$, obtaining a strictly positive vector $\bm{w} \in \mathbbm{R}_+$ and applying the closure operation $\bm{r}=\ClOp(\bm{w})$

\begin{equation}
r_j = 
\begin{cases}
\frac{\kappa}{\kappa + \sum_{i| x_i=0}\delta_i } \delta_j, & \text{if } x_j=0, \\
\frac{\kappa}{\kappa + \sum_{i| x_i=0}\delta_i } x_j, & \text{if } x_j>0.
\end{cases}
\label{eq:simple replacement strategy}
\end{equation}

The main result of \cite{Josep:2003} is the multiplicative replacement strategy. The proposed replacement is 

\begin{equation}
r_j = 
\begin{cases}
\delta_j, & \text{if} x_j=0, \\
\left( 1- \frac{\sum_{i | x_i=0}\delta_i}{\kappa} \right)x_j, & \text{if } x_j>0, 
\end{cases}
\label{eq:multiplicative replacement strategy}
\end{equation}

where $\delta_j$ is the imputed value. The advantages of this method in comparison with the Additive and Simple Replacement Strategy are further explained in \cite{Josep:2003}.

Another method proposed in \cite{Lubbe:2021} is to replace rounded zeros with values drawn from a continuous uniform distribution $U(0.1\cdot DL,DL)$. Setting the lower limit to $0.1\cdot DL$ makes sure that the values are not getting too close to zero and not using a constant prevents underestimation of the variability. They further present the R-package \textit{zCompositions} by \cite{Palarea-Albaladejo:2015}. This package includes some more advanced methods which are based on Markov Chain Monte Carlo (MCMC), the EM algorithm or multiple imputation to perform imputation. Some methods include Model-based multiplicative log-normal imputation, which fits a log-normal distribution for the data, a Data Augmentation (DA) algorithm, which is based on MCMC, and Multiplicative Kaplan–Meier smoothing spline replacement \cite{Palarea-Albaladejo:2015}.



