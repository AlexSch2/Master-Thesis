\section{CoDA}
\label{sec:Coda}

\subsection{Motivation}
\label{sec: Coda Motivation}

One way to see our data is as a compositional time series. The exact definition of compositional will follow later but in general compositional data, which is by nature multivariate, describes relations between the parts instead of absolute values. We transform the data in such a way, that the values of each category can be seen as the relative share of the total amount at the current time. We then predict the relative share of the category for the next point in time. Since we are ultimately interested in the absolute value, we include the total sum of all categories as an additional variable and then use it for calculating the absolute value. This is modelled as the so-called $\Tsp$-Space which will be introduced later. Since VAR models are easy to estimate and interpret and have some beneficial properties with our choice of transformation, we opt to focus on them. One such property is the fact that the VAR model does not depend on the concrete choice of the ilr-transformation \cite{Kynclova:2015}. 

%Beschreiben warum CoDA und wie unsere Daten als compositional Data gesehen werden kann. 

\subsection{Preliminaries}
\label{sec: Coda Preliminaries}
The basis of this section is given by \cite{Kynclova:2015}, \cite{Egozcue:2003} and \cite{Filzmoser:2020}.

CoDA, which is short for "`Compositional Data Analysis"' works with compositional data. The key to compositional data is the fact that the absolute value of its parts is less important than the relative relation of the parts to each other. To define compositional data, we first need to define the $(D-1)$-dimensional simplex,
	\begin{equation*}
	\mathbbm{S}^D := \left\{(x_1,\ldots, x_D)^T: x_i > 0, i =1,\ldots,D; \sum_{i=1}^{D}x_i=\kappa  \right\}
	\label{eq:Simplex Definition}
	\end{equation*}
	
where $\kappa$ is a positive constant \cite{Kynclova:2015}. The choice of $\kappa$ is not relevant, as the relative information in the compositional parts stays the same.  A D-dimensional vector $\textbf{x} = (x_1,\ldots,x_D)^T$ is said to be compositional if it is part of $\mathbbm{S}^D$. Next we can induce a $(D-1)$-dimensional vector space on $\mathbbm{S}^D$ by perturbation and power transformation. For compositions $\textbf{x},\textbf{z} \in \mathbbm{S}^D$ and $a \in \mathbb{R}$ they are defined respectively as \cite{Kynclova:2015}

\begin{equation*}
\textbf{x}\oplus \textbf{z} := \ClOp(x_1z_1,x_2z_2,\ldots,x_Dz_D)^T, \hspace{0.5cm} a \odot \textbf{x} := \ClOp(x_1^a,x_2^a,\ldots,x_D^a)^T.
\label{eq:Simplex Operationen}
\end{equation*}

Here $\ClOp$ is the closure operation that maps each compositional vector from the real value space $\mathbbm{R}_+^D$ into its representation in $\mathbbm{S}^D$

\begin{equation*}
\ClOp \left(\bm{x}\right) := \left(\frac{\kappa x_1}{\sum_{i=1}^D x_i},\ldots, \frac{\kappa x_D}{\sum_{i=1}^D x_i}\right)^T.
\label{eq:Closure Operation}
\end{equation*}


Using $z^{-1} := \ClOp(z_1^{-1},z_2^{-1},\ldots,z_D^{-1})$, the inverse perturbation can be defined as 

\begin{equation*}
\textbf{x} \ominus \textbf{z} := \textbf{x} \oplus \textbf{z}^{-1}
\label{eq: Inverse Perturbation}
\end{equation*}

Now we further define an inner product in order to have an inner product space over the simplex $\mathbbm{S}^D$. For two compositions $\textbf{x},\textbf{z} \in \mathbbm{S}^D$ define the Aitchison inner product as 

\begin{equation*}
\left\langle \textbf{x},\textbf{z} \right\rangle_a := \frac{1}{2D}\sum_{i=1}^{D}\sum_{j=1}^{D}\log(\frac{x_i}{x_j})\log(\frac{z_i}{z_j}).
\label{eq:Aitchon inner product}
\end{equation*}

%With this, the Euclidean vector space structure is induced on the simplex. 
In addition, a norm and distance measure can be defined

\begin{equation*}
\norm{\textbf{x}}_a^2 := \left\langle  \textbf{x},\textbf{x} \right\rangle_a, \hspace{0.5cm} d_a(\textbf{x},\textbf{z}) := \norm{\textbf{x} \ominus \textbf{z}}_a .
\label{eq:Simplex Norm and Distance}
\end{equation*}

This induced geometry is called the Aitchison geometry and it allows us to express a composition $\textbf{x} \in \mathbbm{S}^D$ as a perturbation-linear combination of a basis of $\mathbbm{S}^D$.

However, in order to use standard statistical tools, it is desirable to move from this geometry to the Euclidean real space \cite{Filzmoser:2020}. There are various ways to map the data from the simplex $\mathbbm{S}^D$ to the real space $\mathbb{R}^D$. A review of the most common transformations is provided in the following section.

\subsection{Common Transformations}
\label{sec: Common Transformations}

Let $\textbf{x}, \textbf{z} \in \mathbbm{S}^D$ be D-part compositions.
\subsubsection{alr Coordinates}
\label{sec:alr Coordinates}

The additive log-ratio (alr) Coordinates are defined as

\begin{equation*}
\textbf{z}^{(k)} = alr_k(\textbf{x}) := \left(\log\left(\frac{x_1}{x_k}\right), \ldots, \log\left(\frac{x_{k-1}}{x_k}\right),\log\left(\frac{x_{k+1}}{x_k}\right),\ldots,\log\left(\frac{x_D}{x_k}\right)\right).
\label{eq:alr Coordinates}
\end{equation*}

and map the composition $\textbf{x}$ to the real space $\mathbb{R}^D$. They are mainly mentioned for historic purposes since they are an intuitive way of transformation. However, limitations are posed by their dependence on the choice of the denominator $x_k$ and the fact that they are not orthogonal to each other \cite{Filzmoser:2020}. 

\subsubsection{clr Coefficients}
\label{sec:clr Coefficients}

Let $g(\textbf{x})$ be the geometric mean of $\textbf{x}$. The centered log-ratio coefficients are then defined as 

\begin{equation*}
\textbf{w} = (w_1,\ldots, w_D)^T = clr(\textbf{x}) := \left(\log\left(\frac{x_1}{g(\textbf{x})}\right),\ldots, \log\left(\frac{x_D}{g(\textbf{x})}\right)\right)^T .
\label{eq:clr Coefficients}
\end{equation*}

This transformation maps $\textbf{x}$ into the hyperplane $V = \left\{\textbf{w} \in \mathbb{R}^D: \sum_{i=1}^D w_i=0\right\} \subset \mathbb{R}^D$. Hence the transformed data is constrained, which is emphasised by the term 'coefficient' instead of 'coordinates' \cite{Filzmoser:2020}. It can be shown that the $clr$ transformation is an isometry\cite{Egozcue:2003}. Therefore it holds 

\begin{gather*}
\left\langle  \textbf{x},\textbf{z} \right\rangle_a = \left\langle  clr(\textbf{x}),clr(\textbf{z}) \right\rangle_a, \\
d(\textbf{x},\textbf{z})_a = d(clr(\textbf{x}),clr(\textbf{z})) .
\label{eq:clr Coefficients isometric}
\end{gather*}

\subsubsection{ilr Coordinates}
\label{sec:ilr Coordinates}

The isometric log-ratio (ilr) are closely related to the clr Coefficients. Assume the inverse $clr$ transformation is isometric. Let $\left\{v_1,\ldots,v_{D-1}\right\}$ be an orthonormal base in the hyperplane $V$. Then $\textbf{e}_i = clr^{-1}(v_i), i=1,\ldots,D-1$ is an orthonormal basis of the simplex $\mathbbm{S}^D$. For $\textbf{x} \in \mathbbm{S}^D$, the $ilr$ transformation can then be defined as  

\begin{equation*}
\textbf{u} = ilr(\textbf{x}) = \left(\left\langle \textbf{x},\textbf{e}_1\right\rangle_a,\ldots,\left\langle \textbf{x},\textbf{e}_{D-1}\right\rangle_a\right)^T .
\label{eq:ilr Coordinates}
\end{equation*}

In addition to being isometric, the $ilr$ transformation is also isomorph. Let $\textbf{x}, \textbf{z}$ be two compositions and $a, b  \in \mathbbm{R}$. Then,

\begin{equation*}
ilr(a \odot \textbf{x} \oplus b \odot \textbf{z}) = a \cdot ilr(\textbf{x}) + b \cdot ilr(\textbf{z})
\label{eq:ilr coordinates isomorph}
\end{equation*}

as well as,

\begin{gather*}
\left\langle  \textbf{x},\textbf{z} \right\rangle_a = \left\langle  ilr(\textbf{x}), ilr(\textbf{z}) \right\rangle_a, \\
d(\textbf{x},\textbf{z})_a = d(ilr(\textbf{x}),ilr(\textbf{z})),\\
\norm{x}_a = \norm{ilr(x)} = \norm{u} .
\label{eq:ilr coordinates isometric }
\end{gather*}


From the definition of the ilr coordinates it can be seen that they can be expressed as a linear combination of the basis induced by the clr coefficients as seen above. Let $\textbf{V}$ be a $D \times (D-1)$ matrix with columns $\textbf{v}_i = clr(\textbf{e}_i)$. For a composition $\textbf{x}$ the vector of ilr coordinates associated with $\textbf{V}$ is given by,

\begin{equation*}
\textbf{u}_{\textbf{V}} = ilr_{\textbf{V}}(\textbf{x}) = \textbf{V}^T clr(\textbf{x}) = \textbf{V}^T \log(\textbf{x}). 
\label{eq:ilr coordinates with V}
\end{equation*}

The matrix $\textbf{V}$ is the contrast matrix with the orthonormal basis $(\textbf{e}_i)_{i=1}^{D-1}$ \cite{Egozcue:2003}. A special choice of orthogonal coordinates leads to the coordinates 

\begin{gather*}
ilr(\textbf{x}) = (u_1,\ldots,u_{D-1})^T, \\
u_j = \sqrt{\frac{D-j}{D-j+1}}\log\left(\frac{x_j}{\sqrt[D-j]{\prod_{l=j+1}^D x_l}}\right), \hspace{0.2cm} j=1,\ldots, D-1 .
\label{eq:pivot coordinates}
\end{gather*}

With this choice, the problem of interpretation, which arises from the relative nature of the compositional data and the dimension of the simplex, can be solved. The part $x_1$ is only contained in $z_1$ and therefore contains all relative information of $x_1$ \cite{Filzmoser:2020}. 

To transform the data back in the simplex, the inverse transformation is given by, 

\begin{gather*}
x_1 = \exp\left(\sqrt{\frac{D-1}{D}}u_1\right), \\
x_i = \exp\left( \sum_{j=1}^{i-1}\frac{1}{\sqrt{(D-j+1)(D-j)}}u_j + \sqrt{\frac{D-i}{D-i+1}u_i} \right), \hspace{0.2cm} i=2,\ldots, D-1, \\
x_D = \exp\left(- \sum_{j=1}^{D-1} \frac{1}{\sqrt{(D-j+1)(D-j)}}u_j \right).
\label{eq: Inverse Transformation}
\end{gather*}


\subsection{The VAR Model}
\label{sec:The VAR Model}

Since we have established the basic setting we can now introduce compositional time series (CTS). A CTS $\left\{\bm{x}_t:t=1,\ldots,n \right\}$ can be defined as a series where $\bm{x}_t = \left(x_{1t},\ldots,x_{Dt}\right)^T \in \mathbbm{S}^D$. They are thus characterised by their positive components which sum up to a constant $\kappa_t$ for each point in time $t=1,\ldots,n$ 

\begin{equation*}
\sum_{i=1}^D x_{it} = \kappa_t, \hspace{0.2cm} x_i > 0, i=1,\ldots D \hspace{0.1cm}; t=1,\ldots, n. 
\label{eq:CTS characterisation}
\end{equation*} 

Let $\left\{\bm{Y}_t:t=1,\ldots,T; \bm{Y}_t \in \mathbb{N}_0^K \right\}_f$ be our time series for fridge $f$ and assume that $\bm{Y}_t=(Y_{1t},\ldots,Y_{Kt})^T$ is a K-dimensional compositional vector measured at time $t, t=1,\ldots,T$. Further, let $\textbf{u}_t = ilr(\bm{Y}_t)$ be its $ilr$ transformation determined by the matrix $\textbf{V}$. Then the VAR model with lag order $p$ is given by \cite{Kynclova:2015}

\begin{equation}
\textbf{u}_t = \textbf{c}_{\textbf{V}} + \textbf{A}_{\textbf{V}}^{(1)}\textbf{u}_{t-1} + \textbf{A}_{\textbf{V}}^{(2)}\textbf{u}_{t-2} + \ldots + \textbf{A}_{\textbf{V}}^{(p)}\textbf{u}_{t-p} + \bm{\epsilon}_{t}.
\label{eq:VAR model}
\end{equation}

where $\textbf{c}_{\textbf{V}} \in \mathbb{R}^{K-1}$ is a real vector, $\textbf{A}_{\textbf{V}}^{(i)} \in \mathbb{R}^{(K-1) \times (K-1)}$ are parameter matrices and $\bm{\epsilon}_t$ is a white noise process with covariance matrix $\bm{\Sigma_\epsilon}$. The observation $\textbf{u}_t$ therefore depends on the p past observations $\textbf{u}_{t-1},\ldots,\textbf{u}_{t-p}$. It can be shown, that two VAR(p) models resulting from different $ilr$ transformations are compositionally equivalent, which means that the same predictions are obtained \cite{Kynclova:2015}. 


\subsubsection{Estimation of the VAR Model}
\label{sec: Estimation of the Var Model}


Assuming $T$ observations are used for the model, equation \ref{eq:VAR model} can be written in matrix form as

\begin{gather*}
\textbf{U} = \textbf{ZB} + \textbf{E}, \\
\textbf{U} = (\textbf{u}_1,\ldots,\textbf{u}_T)^T \in \mathbb{R}^{T \times (K-1)}, \\
\textbf{Z} \in \mathbb{R}^{T \times \left[(K-1)p+1\right]} \text{ with } \textbf{Z}_t = \left(1,\textbf{u}_{t-1}^T,\ldots,\textbf{u}_{t-p}^T\right)^T, \\ %von \cite{Kynclova:2015}
\textbf{B} = \left[\textbf{c},\textbf{A}^{(1)}, \ldots, \textbf{A}^{(p)}\right]^T \in \mathbb{R}^{(K-1)p +1 \times (K-1)}.
\label{eq:VAR model matrix}
\end{gather*}


The parameter $\textbf{B}$ can then be estimated separately for each column of $\textbf{U}$ by the ordinary least squares (OLS) method. In addition, if there are no restrictions posed on the parameter, the estimator is equal to the generalised least squares (GLS). If the VAR(p) process is normally distributed and the rows of the error matrix $\textbf{E}$ represent a white noise process, thus $\bm{E} \sim WN(\Sigma)$ where $\Sigma$ is the covariance matrix, then the estimator is also equal to the maximum likelihood (ML) estimator. 
Under these assumptions it can be shown that the OLS estimator is consistent and asymptotic normal \cite{Kynclova:2015} \cite{Luetkepohl:2007}. 


\subsection{$\Tsp$-Spaces}
\label{sec: Tspaces}

Since in our context absolute values should be predicted eventually, we model this information in form of the sum of the original parts. Let $\textbf{x}$ be a D-dimensional compositional vector and define an extended vector space $\Tsp = \mathbb{R}_+ \times \mathbbm{S}^D$.  An element of $\Tsp$ now has the form $\tilde{\bm{x}} = \left[t(\bm{x}),\ClOp\left({\bm{x}}\right)\right] = \left[t_x,\tilde{x}_1,\tilde{x}_2,\ldots,\tilde{x}_D \right]$ where $t(\bm{x})= \sum_{i=1}^D x_i$. This allows us to model the relative structure of the data as well as the total sum of the compositional parts. Often times the logarithm of the sum is taken. In the subsequent analysis, the compositional part $\bm{x}$ is then transformed using one of the possible transformations and then the statistical analysis is performed \cite{Kynclova:2015}. In our case, we have again $\bm{Y}_t=(Y_{1t},\ldots,Y_{Kt})^T$ and hence $\Tsp = \mathbb{R}_+ \times \mathbbm{S}^K$. So $\tilde{\bm{Y}}_t=[t(\bm{Y}_t),\ClOp(\bm{Y}_t)]$ with $t(\bm{Y}_t)=\sum_{k=1}^K Y_{kt}$. 

