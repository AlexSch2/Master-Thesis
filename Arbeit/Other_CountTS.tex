%\section{Other Methods}
%\label{sec: Other methods}

\section{GARCH Models}
\label{sec: Garch Models}

INGARCH models are structurally derived from the generalised autoregressive conditional heteroscedasticity (GARCH) models, which themselves are generalisations of the autoregressive conditional heteroscedasticity (ARCH) model. ARCH models, which were first developed by Engle \cite{Engle:1982} in an economic context, model the variance conditional on past values. Let $\left\{Y_{kt}:t=1,\ldots T; Y_{kt} \in \mathbb{N}_0\right\}_f$ be the univariate time series for category $k$ for $k=1,\ldots,K$ and fridge $f$ and $\SigA_{k,t}$ be the information available at time $t$. Then the ARCH(1) model is given by \cite{Engle:1982}

\begin{equation}
\begin{gathered}
Y_{kt} | \SigA_{k,t-1} \sim N(0,h_{kt}),\\
h_{kt} = a_0 + a_1 Y_{k,t-1}^2,
\label{eq:ARCH model}
\end{gathered}
\end{equation}

with $a_0\geq0$, $a_1>0$. 

The variance function can be generally formulated as $h_{kt} = h(Y_{k,t-1},\ldots,Y_{k,t-p},\bm{a})$, where $\bm{a}=(a_1,\ldots,a_p)^T\geq 0$ is the parameter vector with $a_p>0$, and $p \in \mathbbm{N}$ is the order of the ARCH process. 
The GARCH model generalises this approach by adding the past variances as another source of information. The GARCH(p,q) model for non-negative parameters $a_0>0$, $\bm{a}=(a_1,\ldots,a_q)^T\geq 0$ and $\bm{b}=(b_1,\ldots,b_p)^T\geq0$ with $p,q \in \mathbbm{N}$, $p\geq0, q>0$ is given by \cite{Bollerslev:1986}

\begin{equation}
\begin{gathered}
Y_{kt} | \SigA_{k,t-1} \sim N(0,h_{kt}); \forall t \in \mathbbm{N}, \\
\mathbbm{V}[Y_{kt}|\SigA_{k,t-1}]=h_{kt} = a_0 + \sum_{i=1}^q a_i Y_{k,t-i}^2 + \sum_{j=1}^p b_j h_{k,t-j};\forall t \in \mathbbm{N}.
\label{eq:GARCH model}
\end{gathered}
\end{equation}

Other distributions than the normal distributions can be taken as well. 

\subsection{Parameter Estimation and Forecasting}
\label{sec: GARCH Parameter Estimation and Forecasting}

Estimation of the parameters can be done with maximum likelihood and an iterative algorithm. First, the model is rewritten and the logarithm of the likelihood function is taken. Second, after differentiation with respect to its variance and mean parameters, the Berndt, Hall Hall and Hausman algorithm \cite{Berndt:1974}is used to obtain the maximum likelihood estimates. Further details and assumptions can be found in \cite{Bollerslev:1986}. 

If one is interested in forecasting $Y_{kt}$, then the minimum mean squared one-step error forecast is $\mathbbm{E}[Y_{k,t+h}|\SigA_{k,t}]=0$ where $\SigA_{k,t}$ is the information available at time $t$. One should note, that the forecast is independent of the model parameters. If the conditional variance for should be forecasted, the parameters are estimated and the known values are plugged in. For $h>1$, $h$-step predictions are computed recursively with plugging in the forecasts for $h-1,h-2,\ldots$ in the model \cite{Zivot:2009}. 


\subsection{Testing for GARCH Models}
\label{sec: Testing for GARCH models}

To decide whether to use a GARCH model, one can test for volatility or the validity of GARCH models in general. In the original paper \cite{Bollerslev:1986}, the author suggests a Lagrange multiplier test. Other popular tests include the Box–Pierce–Lung‐type portmanteau tests and residual‐based diagnostics \cite{Hong:2017}. The authors in \cite{Hong:2017} present further methods. 


\subsection{Applications}
\label{sec: Garch Applications}

The introduction of ARCH and subsequently GARCH models in the 1980s has been revolutionary. ARCH models have originally been introduced for modelling macroeconomic key figures such as inflation rates but since then have been used in a variety of fields. GARCH models generalised the ARCH model approach to allow the modelling of a more flexible lag structure \cite{Bollerslev:1986}. They have found wide applications in finance mathematical problems, especially for the modelling of a changing variance and volatility in financial markets. They are often used to estimate volatility of various financial instruments. 

Since the ARCH and GARCH models are used to model and forecast volatility or the conditional variance, but not values, we will not use it in our application. In addition, the INGARCH model also accounts for the discrete nature of our data, which makes it the preferred choice. 


\section{Naive Random Walk}
\label{sec: Naive Random Walk}

The Naive Random Walk model is one of the simplest and most comprehensive forecasting models, which makes it a popular benchmark model. In addition, it is what is currently employed, so using it enables us to directly see if our models outperform the current model. It assumes that the 1-step difference between two values is i.i.d distributed with mean 0. Let $\left\{Y_{kt}:t=1,\ldots T; Y_{kt} \in \mathbb{N}_0\right\}_f$ be our univariate time series. Then the Naive Random Walk model is given as

\begin{equation}
Y_{k,t+1}= Y_{kt} + \epsilon_{kt},\hspace{0.2cm} k=1,\ldots,K \hspace{0.1cm}, 
\label{eq: Random Walk Model}
\end{equation}
 
where $\epsilon_{kt} \sim WN(\sigma^2)$ is a white noise process with variance $\sigma^2 \in \mathbb{R}_+$. It can be shown easily, that the optimal 1-step ahead forecast with regards to the mean squared error (MSE) is given by

\begin{equation}
\hat{Y}_{k,t+1}= Y_{kt} ,\hspace{0.2cm} k=1,\ldots,K ,
\label{eq: Random Walk Model Prediction}
\end{equation}

where $\hat{Y}_{k,t+1}$ is the predicted value at time $t$. In other words, the predicted value is the last known value.  


\section{Zero-Inflated Models}
\label{sec: Zim}

Since we encounter a large number of zeros, we also consider zero-inflated models. Zero inflation means that the proportion of observed zeros is bigger than that of the underlying distribution and hence would not be expected. The idea of zero-inflated models is to add a degenerated distribution with mass at zero to the probability mass function, which enables one to explain the large amount of zero values. The probability mass function of a $ZIP(\lambda,\omega)$ distribution for a random variable $Y$ is defined as \cite{Zhu:2012}

\begin{equation}
\mathbb{P}(Y=y) = \omega \delta_{y,0}+ (1-\omega) \frac{\lambda^y \exp(-\lambda)}{y!}, \hspace{0.2cm} y \in \mathbb{N}_0.
\label{eq:ZIP Distribution}
\end{equation}

where $0 < \omega < 1$ is the zero-inflation parameter, $\lambda$ is the Poisson parameter and $\delta_{y,0}$ is the Kronecker delta for which $\delta_{y,0}=1$ if $y=0$ and $\delta_{y,0}=0$ else. This way our zeros can come from two different sources \cite{Zhu:2012}. The first part of equation \ref{eq:ZIP Distribution} $\delta_{y,0}$ is the degenerated point mass distribution.

Now we can define the Zero-Inflated Poisson (ZIP) INGARCH(p,q) as

\begin{equation}
\begin{gathered}
\label{eq:ZIP model}
Y_{kt} | \SigA_{k,t-1} \sim ZIP(\lambda_{kt},\omega_k); \forall t \in \mathbb{N}, \\
\mathbb{E}\left[Y_{kt} | \SigA_{k,t-1} \right] = \lambda_{kt} = \beta_0 + \sum_{i=1}^p\beta_i Y_{k,t-i} + \sum_{j=1}^q\alpha_j \lambda_{k,t-j},
\end{gathered}
\end{equation}

with $0<\omega<1$, $\beta_0>0$, $\beta_i\geq 0$, $\alpha_j \geq 0$ for $i=1,\ldots,p$, $j=1,\ldots,q$, $p\geq 1$, $q\geq 0$ and $\SigA_{k,t-1}$ is the $\sigma$-field generated by $\left\{X_{t-1},X_{t-2},\ldots\right\}$ \cite{Zhu:2012}. If $\omega =0$ then we get the normal INGARCH(p,q) model discussed above. It can be shown that the conditional mean and variance are given by

\begin{equation}
\mathbb{E}[Y_{kt} | \SigA_{k,t-1}] = (1-\omega_k)\lambda_{kt}, \hspace{1cm} \mathbb{V}[Y_{kt} | \SigA_{k,t-1}] =(1-\omega)\lambda_{kt}(1+\omega \lambda_{kt}),
\label{eq:Conditional Mean and Variance ZIP}
\end{equation}

which implies $ \mathbb{V}[Y_{kt} | \SigA_{k,t-1}] > \mathbb{E}[Y_{kt} | \SigA_{k,t-1}]$ \cite{Zhu:2012}. This means that model \ref{eq:ZIP model} can handle overdispersion in our data. More details about zero-inflated models and especially the zero-inflated INGARCH(p,q) model can be found in \cite{Zhu:2012}.

However, due to a lack of available R-packages for zero-inflated Poisson INGARCH models, we use a zero-inflated Poisson autoregressive model. We again assume that our data is conditionally $ZIP(\lambda_{kt},\omega_{kt})$ distributed. For the parameters $\lambda_{kt}$ and $\omega_{kt}$, the ZIP autoregressive model is given by \cite{Lambert:1992}

\begin{equation}
\begin{gathered}
\log(\lambda_{kt}) = \sum_{i=1}^p\beta_i b_{k_{ti}},\\ %\bm{B}^T_{k,t-1} \bm{\beta},\\
\log\left(\frac{\omega_{t}}{1-\omega_{kt}}\right)=\sum_{i=1}^m\gamma_i z_{k_{tj}},%\bm{Z}_{k,t-1}^T\bm{\gamma},
\label{eq:ZIP Autoregressive model}
\end{gathered}
\end{equation}

where $\bm{\beta} = (\beta_1,\ldots,\beta_p)^T$ and $\bm{\gamma}=(\gamma_1,\ldots,\gamma_m)^T$ are the parameters to be estimated and the vectors $\bm{B}_{k_t}=(b_{k_{tj}})$ and $\bm{Z}_{k_t}=(z_{k_{tj}})$ are the explanatory covariates. In model \ref{eq:ZIP Autoregressive model} a logit link function has been used although, it can be replaced with other link functions like the probit or log link. 

In our case we regress on the past values of our time series. In that case model \ref{eq:ZIP Autoregressive model} becomes

\begin{equation}
\begin{gathered}
\log(\lambda_{kt}) = \beta_1 + \beta_2 Y_{k,t-1},\\%(1,Y_{k,t-1}) \bm{\beta},\\
\log\left(\frac{\omega_{kt}}{1-\omega_{kt}}\right)= 1 \cdot \gamma.
\label{eq:ZIP Autoregressive model timeseries}
\end{gathered}
\end{equation}

%Hence we have $\bm{B}_{t} = (1,Y_{k,t})$ and $\bm{Z}_{t} = 1$. 

\subsection{Parameter Estimation and Forecasting}
\label{sec: ZIM Parameter Estimation and Forecasting}

Parameter estimation can be done with the EM algorithm. Further details can be found in \cite{Lambert:1992}.

The one step ahead predictor is again given by the conditional expectation $\mathbb{E}[Y_{kt} | \SigA_{k,t-1}] = (1-\omega_k)\lambda_{kt} $ with the estimated coefficients plugged in. 

\section{Log-Linear Models}
\label{sec: Log-Linear Models}

As mentioned in section \ref{sec:Ingarch Motivation} we also investigate log-linear models. These models are structurally very similar to the normal INGARCH(p,q) model, only with a logarithmic link function. They have the form 

\begin{equation}
\begin{gathered}
Y_{kt} | \SigA_{k,t-1} \sim P(\lambda_{kt}); \forall t \in \mathbb{N}, \\
\nu_{kt}= \log(\lambda_{kt}) = \beta_0 + \sum_{i=1}^p\beta_i \log(Y_{k,t-i}+1) + \sum_{j=1}^q\alpha_j \nu_{k,t-j}.
\label{eq:Log-Linear model}
\end{gathered}
\end{equation}

The past values get transformed by $h(x)=\log(x+1)$ to get them on the same scale as $\nu_{kt}$ and avoid zero values in the logarithm \cite{Liboschik:2016,Fokianos:2011}. We consider the Log-Linear model because it provides solutions to at least two drawbacks from the INGARCH(p,q) model. First, as a result of the definition of the parameter space \ref{eq:Ingarch parameter space}, we have $0 < \sum_{i=1}^p\beta_i + \sum_{j=1}^q\alpha_j < 1$ and hence it follows for $h\in \mathbb{N}$ that $Cov(Y_{k,t+h},Y_{kt})>0$. Second, when we include covariates, they can only have a positive regression term because otherwise the mean $\lambda_{kt}$ becomes negative \cite{Fokianos:2011}. However, in the log-linear case we can extend this to

\begin{equation}
\begin{gathered}
Y_{kt} | \SigA_{k,t-1} \sim P(\lambda_{kt}); \forall t \in \mathbb{N}, \\
\nu_{kt}= \log(\lambda_{kt}) = \beta_0 + \sum_{i=1}^p\beta_i \log(Y_{k,t-i}+1) + \sum_{j=1}^q\alpha_j \nu_{k,t-j} + \bm{\eta}^T\textbf{X}_{kt}.
\label{eq:Log-Linear model external factors}
\end{gathered}
\end{equation}

with $\bm{\eta} \in \mathbb{R}^r$. Additionally, because of the updated definition of the parameter space \ref{eq:Parameter Space log-linear}, it also allows for negative autocorrelation \cite{Liboschik:2016}. 

\subsection{Parameter Estimation and Forecasting}
\label{sec: Log-Linear Parameter Estimation and Forecasting}

The parameter estimation for the log-linear model is done similarly to the INGARCH model in \ref{sec: Estimation of the Ingarch Model}. Only the parameter space $\Theta$ is different

\begin{equation}
\Theta = \left\{ \bm{\theta} \in \mathbb{R}^{p+q+r+1}: \abs{\beta_1},\ldots,\abs{\beta_p},\abs{\alpha_1},\ldots,\abs{\alpha_q} < 1, \abs{\sum_{i=1}^p\beta_i + \sum_{j=1}^q\alpha_j } < 1 \right\}.
\label{eq:Parameter Space log-linear}
\end{equation}

Just like parameter estimation, forecasting is also performed in the same way as the INGARCH model. The optimal one-step ahead prediction with regards to the mean squared error is given by the conditional expectation $\lambda_{k,t+1}=\mathbb{E}\left[Y_{k,t+1} | \SigA_{kt} \right]$. The h-step ahead predictions for $h>1$ are calculated iteratively again \cite{Liboschik:2016}. 

Log-Linear Models are further discussed in \cite{Fokianos:2011,Woodard:2011,Douc:2013}.


%\section{Vector Generalised Additive Models}
%\label{sec:Vgam}
%
%Because we work with multivariate count data, we also look at vector generalised additive models (VGAMs) which extend generalised additive models (GAMs) to higher dimensions. GAMs allow us to reveal and model non-linear relationship in our data, as opposed to linear models or generalised linear models \cite{Yee:1996}. Let $y$ be a univariate response with a distribution in the exponential family and mean $\mu$. Further take a p-dimensional covariate vector and $\bm{x}=(x_1,\ldots,x_p)^T$. Then the generalised additive model (GAM) is given by
%
%\begin{equation}
%g(\mu) = \nu(\bm{x}) = \beta_0 + f_1(x_1) + \ldots f_p(x_p),
%\label{eq:Gam}
%\end{equation}
%
%with $f_j$ being arbitrary smooth functions \cite{Yee:1996}.
%To extend this model to the multivariate case, we replace the functions $f_j$ with vector functions. Let $\bm{f}_k(Y_{kt}) = (f_{(1)k}(Y_{kt}),\ldots,f_{(M)k}(Y_{kt}))^T$ with $M \in \mathbbm{N}$ be an arbitrary smooth vector function. Then the vector generalised additive model is given by
%
%\begin{equation}
%\mathbb{E}[\bm{Y}_t] = \bm{\beta}_0 + \sum_{k=1}^K\bm{f}_k(Y_{k,t-1}),
%\label{eq:Vgam}
%\end{equation}
%
%where $\mathbb{E}[\bm{Y}_t] = (\mathbb{E}[Y_{1t}],\ldots,\mathbb{E}[Y_{Kt}])^T$ \cite{Yee:1996}. Further theoretical background about VGAMs are given in \cite{Yee:1996,Yee:2015,Wood:2004}.

\section{AR Models}
\label{sec: Ar Models}

Autoregressive models of order p (AR(p))  are one of the most simple time series models, which makes them very popular. For $\bm{a}=(a_1,\ldots,a_p)^T \in \mathbbm{R}^p$ and a white noise process $\epsilon_{kt} \sim WN(\sigma^2)$, called innovations, they are defined as 

\begin{equation}
Y_{kt} = a_1Y_{k,t-1} + \ldots + a_pY_{k,t-p} + \epsilon_{kt},
\label{eq: Ar model}
\end{equation}

where $\left\{Y_{kt}:t=1,\ldots T; Y_{kt} \in \mathbb{N}_0\right\}_f$ is again our univariate time series. The multivariate version is defined as 

\begin{equation}
\bm{Y}_{t} = \bm{a_1}Y_{t-1} + \ldots + \bm{a_p}Y_{t-p} + \bm{\epsilon}_{t},
\label{eq: Var Model}
\end{equation}

where $\bm{a}_j \in \mathbbm{R}^{k \times k}$ and $\bm{\epsilon}_t \sim WN(\Sigma)$.% with $\Sigma \in \mathbbm{R}^{k \times k}$. 

\subsection{Parameter Estimation and Forecasting}
\label{sec: AR Estimation and Forecasting}

The simplicity of AR models makes parameter estimation and forecasting easy. There are various ways to estimate the parameters in model \ref{eq: Var Model} such as the Yule-Walker equations, the ordinary least squares (OLS) estimator and if the innovations $(\bm{\epsilon}_t)$ are multivariate normal distributed, then the maximum likelihood estimator can be used as well. Further properties and comparison of their estimators can be found in \cite{Scherrer:2021}. 

Like parameter estimation, forecasting is also simple in the AR model. Using the mean squared error as a measure, only considering affine forecasts and using $m \in \mathbbm{N}$ past values, hence 

\begin{equation}
\hat{\bm{Y}}_{t+h} = \bm{c}_0 + \bm{c}_1\bm{Y}_{t} + \ldots + \bm{c}_m\bm{Y}_{t-m+1},
\label{eq:Forecasting general}
\end{equation}

we get that the optimal one-step ahead prediction is simply 

\begin{equation}
\hat{\bm{Y}}_{t+1} = \bm{a}_1\bm{Y}_{t} + \ldots + \bm{a}_p\bm{Y}_{t-m+1},
\label{eq:AR 1step Forecasting}
\end{equation}

for $m>p$ \cite{Scherrer:2021}. For $h>1$, one simply continues recursively, using $\hat{\bm{Y}}_{t+1}$, $\hat{\bm{Y}}_{t+2},\ldots$ .

\subsection{Testing for AR Models}
\label{sec: Testing for ar models}

To test whether or not a time series follows an AR(p) process, the estimates of the white noise process $\hat{\epsilon}_t$ can be used. These estimates should follow a white noise process and hence should show no signs of serial correlation. Popular tests are the Portmanteau and the Breusch-Godfrey Test \cite{Scherrer:2021}. 

The Portmanteau Test tests the null hypothesis $H_0: \mathbbm{E}[\bm{\epsilon}_t \bm{\epsilon}_{t-m}^T]=0$, i.e. if the estimated innovations are uncorrelated. Under the assumption that $(\bm{Y}_t)_{t=1}^T$ is an AR(p) process and an AR(p) model has been fit, the used test statistic converges against a chi-squared distribution \cite{Scherrer:2021}. 

The Breusch-Godfrey Test tests if the coefficients $(b_1,\ldots,b_h)$ in the model 

\begin{equation}
\bm{\epsilon}_t = d_1\bm{\epsilon}_{t-1} +\ldots d_h \bm{\epsilon}_{t-h} + \bm{\eta}_t,
\label{eq:Breusch-Godfrey Test model}
\end{equation}

are zero, i.e. if process $(\bm{\epsilon}_t)$ follows an AR(h) structure or not. Under the null hypothesis the test statistic follows a chi-squared distribution again \cite{Scherrer:2021}. 

\section{INAR(p) Models}
\label{sec: Inar Models}

Integer valued autoregressive models of order p (INAR(p)) are another option to handle univariate count data. To define them, we first need to define the generalised thinning operator. Take an integer-valued, non-negative random variable $X$ and $\alpha \in [0,1]$. Further, take a sequence of i.i.d. integer-valued, non-negative random variables $(Z_i)_{i=1}^X$ with finite mean $\alpha$ and variance $\sigma^2<\infty $ which are independent of $X$. Then the generalised thinning operator $\circ$ is defined as

\begin{equation}
\alpha \circ X = \sum_{i=1}^X Z_i .
\label{eq:Thinning operator}
\end{equation}

The sequence  $\left\{Z_i \right\}_{i=1}^X$ is called the counting series of $X$ \cite{Silva:2005}. 

We can then define the INAR(p) model for a positive integer-valued time series $\left\{X_t \right\}$ as

\begin{equation}
X_t = \alpha_1 \circ X_{t-1} + \alpha_2 \circ X_{t-2} + \ldots + \alpha_p X_{t-p} +\epsilon_t ,
\label{eq:Inar(p) model}
\end{equation}

where

\begin{enumerate}
	\item $(\epsilon_t)$ is a sequence of integer-valued i.i.d. random variables, called innovations, with finite first, second and third moment, 
	\item $\alpha_i \circ X_{t-i}$ for $i= 1,\ldots,p$ and $(Z_j)$ for $j=1,\ldots,X_{t-i}$ are mutually independent, independent of $(\epsilon_t)$ and it holds $\mathbb{E}[Z_{i,j}]=\alpha_i$, as well as $\mathbb{V}[Z_{i,j}] = \sigma_i^2$ and $\mathbb{E}[Z_{i,j}^3] = \gamma_i$,
	\item $\alpha_i \in (0,1]$ for $i=1,\ldots,p-1$ and $0 < \alpha_p < 1$,
	\item $\sum_{j=1}^p \alpha_j < 1$ \cite{Silva:2005}. 
\end{enumerate}


The last condition ensures the existence and stationary of the process. 

Let $\left\{Y_{kt}:t=1,\ldots T; Y_{kt} \in \mathbb{N}_0\right\}_f$ be again the univariate time series for category $k$ for $k=1,\ldots,K$ and fridge $f$. Then the INVAR(p) model is given by

\begin{equation}
Y_{kt} = \alpha_1 \circ Y_{k,t-1} + \alpha_2 \circ Y_{k,t-2} + \ldots + \alpha_p Y_{k,t-p} +\epsilon_{kt}.
\label{eq:Inar(p) model ts}
\end{equation}

For simplicity, we will consider INAR(1) models, although the optimal choice of the lag is something that could be further investigated. 

\subsection{Distributional Assumptions}
\label{sec: Inar Distributional assumptions}

While we will mainly assume that the innovations $(\epsilon_t)$ follow a Poisson distribution, they can also follow other distributions. One interesting option is, that one can choose a zero-inflated distribution. This could make the model adequate for our data. 


\subsection{Parameter Estimation and Forecasting}
\label{sec: Inar Parameter Estimation and Forecasting}

Parameter estimation can be done in several ways. Possible methods are: moment based estimators (MM), regression based or conditional least squares (CLS) and maximum likelhood (ML) based estimators. Especially for the Poisson model, those methods have been studied in detail in literature \cite{Silva:2005}. 

The authors in \cite{Silva:2005} present two types of forecasting methods for INAR(1) models. The first approach is a classical method for performing predictions in a time series context and makes use of the conditional expectation. It was obtained by \cite{Bre:1993} and \cite{Freeland:2004}. Assuming that $(\epsilon_t) \sim_{i.i.d} P(\lambda)$, the $h$-step ahead predictor, for $h\in \mathbbm{N}$, based on $n$ past observations $\bm{Y}_k=(Y_{k1},\ldots,Y_{kn})$ is given by

\begin{equation}
\hat{Y}_{k,n+h} = \mathbb{E}[Y_{k,n+h} | \bm{Y}_k] = \alpha^h \left[Y_{kn}- \frac{\lambda}{1-\alpha} \right] + \frac{\lambda}{1-\alpha}.
\label{eq:Forecasting Classic}
\end{equation}


However, this forecast hardly ever produces integer values. One option to counter this problem, is to take the value which minimises the absolute expected error $\mathbbm{E}[\abs{Y_{k,n+h} - \hat{Y}_{k,n+h}}|Y_{k,n}]$ instead of the MSE. This turns out to be the median $\hat{m}_{n+h}$ of the h-step ahead conditional distribution of $Y_{k,n+h}|Y_{k,n}$ \cite{Silva:2005,Freeland:2004}. Another option is a bayesian approach presented in \cite{Silva:2005}. It is based on the assumption that both, the future prediction $Y_{k,n+h}$ and the vector of unknown parameters $\bm{\theta}=(\alpha,\lambda)$ are random \cite{Silva:2005}. Since the complexity posterior probability density function makes it difficult to work with it directly, a sampling algorithm can be deployed for estimation. The details are again given in \cite{Silva:2005}. The estimator for the conditional expectation is then given by

\begin{equation}
\hat{Y}_{k,n+h}= Y_{kn}\left(\frac{1}{m} \sum_{i=1}^m\alpha_i^m\right) + \left(\frac{1}{m} \sum_{i=1}^m \frac{1-\alpha_i^h}{1-\alpha_i}\lambda_i\right),
\label{eq:Forecasting Bayesian}
\end{equation}

where $m$ is the sampling size and the pairs $(\alpha_i,\lambda_i)$ for $i=1,\ldots,m$ are the sampled parameters. 

\subsection{Testing for INAR(1) Models}
\label{sec:Testing for INAR(1) Models}

To test the adequacy of the INAR(1) model, there are again various options. 

Parametric re-sampling is a popular method. The idea is to generate data with the help of the fitted model, construct the empirical distribution of the functional of interest and check if the original sample is a reasonable point within that empirical distribution \cite{Silva:2005}. 

Residual based methods are based on the Pearson residuals defined by 

\begin{equation}
r_{kt} = \frac{Y_{kt}-\mathbbm{E}[Y_{kt}|Y_{k,t-1}]}{\mathbbm{V}[Y_{kt}|Y_{k,t-1}]^{1/2}},
\label{eq:Pearson residuals}
\end{equation}

where estimated quantities are plugged in. If the model is specified correctly, the residuals should have mean zero, variance one and no significant serial correlation \cite{Silva:2005}. 

Another option is based on predictive distributions where an adjusted probability integral transform (PIT) is used. Further details can be found in \cite{Silva:2005}. 

\subsection{Difference to AR(p) Models}
\label{sec: Difference to AR models}

%Using other defintion of INAR(p) model. 
%While structurally the AR(P) and then INAR(P) model look similar they have different properties. The only exception is for $p=1$. The authors in \cite{McKenzie:1986} and \cite{Alzaid:1988} noted that the aside from their structural similarity, those two processes also show the same autocorrelation and regression behaviour \cite{Alzaid:1990}. However, for $p>1$ the similarities end at the structure. For an INAR(p) process, the components $\alpha_i \circ Y_{t,k}$ of $Y_{t,k}$ have a mutual dependence structure for $i=1,2,\ldots,p$ which induces a moving-average structure. In the AR(p) process they are only connected by the presence of $Y_{t,k}$. Because of this additional dependence, it can be shown that the autocorrelation behaves more like a standard ARMA(p,p-1) process \cite{Alzaid:1990}. 

Depending on the definition of the INAR(p) model, the degree of similarity varies. If one follows the definition of \cite{Guan:1991}, which is the one given in equation \ref{eq:Inar(p) model}, then the autocorrelation function follows that of an AR(p) process \cite{Oliveira:2005}. However, the authors in \cite{Alzaid:1990} propose a different definition. In their work, given $Y_{tk}=y_{tk}$, the conditional distribution of $(\alpha_1 \circ Y_{tk}, \ldots, \alpha_p \circ Y_{tk})$ is multinomial with parameters $( \alpha_1,\ldots,\alpha_p,y_{tk})$ and is independent of the history of the process. Under those assumptions, the components $\alpha_i \circ Y_{t,k}$ of $Y_{t,k}$ for $i=1,2,\ldots,p$  have a stronger mutual dependence structure than the corresponding AR(p) process and induce a moving-average structure \cite{Alzaid:1990}. Because of this additional dependence, it can be shown that the autocorrelation behaves more like a standard ARMA(p,p-1) process \cite{Alzaid:1990}. 